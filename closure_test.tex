\section{Monte Carlo approach to fitting data}
% Generally propagating uncertainty such that the result of the fit contains
% Information on possible outcomes of a fit.
Without making this too much about PDFs want to say
\begin{itemize}
    \item Have some data with some error
    \item use that to make an assumption of the distribution of the data central values
    \item consider the use case of our product - people make new predictions and
    test consistency with input assumptions.
    \item we want to provide a distribution of possible outcomes of a fit based
    taking into account the experiment uncertainty, fitting inefficiency etc.
    \item one way of doing this is to instead provide a sample of that
    distribution in the form of MC replicas, from which somebody can generate
    a sample of new predictions
    \item advantage is that we don't need to be able to parameterise the
    distribution in function space, so we don't have to rely on using a simple
    parameterisation and assuming gaussianity (hessian approach)
    \item does this actually work in theory? Test with simple models, work up
    to actual fit, try and determine potential pitfalls along the way.
    \item the aim of this paper is not to decide whether or NNPDF4.0 is a success
    but to present a set of tools which can determine if the fit is successful
    with some input assumptions being satisfied.
\end{itemize}
The next section I think should be assumptions of the data, which is consistent
throughout. Then we can talk about what is minimised in the fit. Possibly
introduce the vector notation.

\section{Fitted data}
If we consider experimental data which we assume
to be multigaussian then the experimental central values, $\levone$, are given by
\begin{equation}\label{eq:levelonedata}
    \levone_{i} = \law_i + \shift_i,
\end{equation}
where $i$ is the index of the data point.
In other words, the experimental values have been shifted away from the true
values given by nature, $\law$, by some shift, $\shift$. The vector of shifts
is drawn from
the multigaussian $\mathcal{N}(0, \cov)$ where $\cov$ is the experimental
covariance matrix. Equation \eqref{eq:levelonedata} is our interpretation of the
experimental data, which is often provided in different forms. In the case that
the experimental data is a set of central values and a covariance matrix then
this interpretation appears entirely natural.
% we never talk about normalisation uncertainties.

The pseudodata which will be fitted is obtained by adding Monte Carlo
noise, $\noise^{\repind}$, on top of the experimental central values
\begin{equation}
    \levtwo^{\repind}_{i} = \law_i + \shift_i + \noise^{\repind}_{i},
\end{equation}
where the replica index $k$ refers to each replica having a noise vector drawn
independently from $\mathcal{N}(0, \cov)$. The sampling of pseudodata permits
correlations between individual data points through the covariance matrix,
however each replica is sampled independently from the others.
% Still wonder if we should use 2C in the chi2 which is minimised by each replica.
% what happens in the polynomial model?
Each pseudodata replica gets independently fitted by a replica of the model.
The end product of the fit is a sample of model replicas which are sampled from
the distribution of possible models given the distribution of possible
pseudodata replicas. Considering the observables generated from the model replicas
for the fitted data, $\model^{\repind}$, the purpose of the level 2 noise
is to induce a dispersion in the predictions. The parameters for each of the
replicas will consequently have a dispersion and it seems reasonable to assume
that observables generated from the model for new data which was not included
in the fit will also have a dispersion, which can later be interpreted as the
contribution to the uncertainty of that prediction from the model.

\section{Data and Fitting}
% Should this section go later? I think so.
When fitting experimental data we vary the parameters of a set of PDF replicas
at the initial scale such that the $\chi^2$ is minimised between the
corresponding theory predictions and a generated pseudodata replica. A set of
PDFs usually refers to a set of seperate continuous functions, one for each
flavour of PDF in a particular basis. In this specific study, fits performed
with \nfit\ parameterise the set of PDFs as a single neural network which takes
as input $x$ and $\ln x$ and returns 8 outputs, one for each flavour in the
fitting basis, multiplied by some preproccessing exponents. The output for a
single flavour $j$ is
\begin{equation}
    NN(x, \ln x)_j * x^{1-\alpha_j} * (1-x)^{\beta_j},
\end{equation}
where each flavour has it's own preproccessing exponents $\alpha$ and $\beta$,
parameters that are varied in these fits, and $NN(x, \ln x)_j$ is the
$j^{\rm th}$ output from the neural network.
When an experiment is included in an NNPDF fit, we take the published
experimental central values and uncertainties (statistical and systematic)
and use these pieces of information to generate the pseudodata.
The pseudodata replica is generated
through Monte Carlo sampling by applying noise to the experimental
central values.
After fitting many sets of PDF replicas (usually of order 100 sets),
each set to an independently generated pseudodata replica, we have an ensemble of
PDF replicas.
The aim of this methodology, is to propagate the various sources of
uncertainty involved with fitting PDFs into the functional PDF space in a faithful
manner. This means that the distribution from which the PDF replicas are drawn
from should be representative of the probability distribution of the true
underlying PDF.

The closure test was introduced alongside NNPDF3.0 and will be described below.
At this stage it's important to note that the closure test may serve 3 purposes:
We can test whether or not the ensemble of PDF replicas does reflect the
probability distribution of the true underlying PDF; we can compare two different
fitting methodologies and use estimators to determine which one performs better;
we can try to understand different elements of our own methodology, such as
the different contributions to the PDF uncertainty. The PDF uncertainty will
be used as short-hand for referring to the distribution of replicas for a given
fit, we consider this the PDF uncertainty because if the distribution
of the true underlying PDF is reflected by the distribution of replicas for
a given fit, then the standard deviation of the replicas in PDF space or the
theory predictions obtained from those replicas in data space represents
the uncertainty of the prediction having performed a fit.

\subsection{Closure test.}

In a closure test, instead of assuming that the experimental central values are
statistically consistent with an underlying law, as in \eqref{eq:levelonedata},
we use a pre-existing PDF as an underlying law and generate the experimental
central value, by drawing the vector of shifts from the distribution
described by the experimental uncertainties. We refer to the predictions
obtained from the underlying law as level 0 data, $\law$, and the generated
experimental central values as level 1 data, $\levone$. Finally the pseudodata
replicas are generated from the $\levone$ data, identically to a fit to
a fit to experimental data. We refer to the pseudodata replicas as level 2
data, $\levtwo$.

The parameters of each PDF replica are determined by minimising $\chi^2$ between
the predictions of each PDF replica and the corresponding pseudodata replica,
as in a fit to experimental data:
\begin{equation}
    \repchis = \frac{1}{\ndata} \sum_{ij} \diffreptwo_i \invcov{ij} \diffreptwo_j,
\end{equation}
where $\model^{\repind}_i$ is the prediction for $i^{\rm th}$ datapoint, from
the $k^{\rm th}$ set of PDFs. After fitting many replicas, the quality of a fit
is often determined by considering the $\chi^2$ between the experimental central
values and the expectation value of the theory predictions
\begin{equation}\label{eq:centralchi2}
    \chi^2 = \frac{1}{\ndata} \sum_{ij} \diffcentone_i \invcov{ij} \diffcentone_j,
\end{equation}
where $\erep{\cdot}$ denotes the mean value across replicas, so $\erep{g}$ is
the mean of the theory predictions across replicas. This $\chi^2$ is a measure
of the difference between the expectation value of the prediction and the
experimental central values in units of the covariance.

In a fit to experimental data we are limited to assessing the performance of a fit
using this quantity because we don't
have knowledge of the underlying theory predictions, $\law$. What's more, the
assumption that all datapoints across the global dataset used in a fit are
mutually consistent with being generated from a single underlying law from
the distribution defined by the uncertainties might not hold, often
referred to as tension in the data. In particular, our model for generating pseudodata
is based on rather coarse information and despite our best efforts we don't know
if the distribution of our pseudodata truly represents the shape of the distribution
the of the experimental central value, merely that we reproduce the first
two moments of the distribution. Accepting these as unavoidable flaws in any
fit to experimental data, we use the closure test to bypass these and check that
at least in the ideal case where there is no tension in the data, and it is
generated faithfully, that the fitting methodology performs well.

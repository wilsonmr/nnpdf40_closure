\section{Data and Fitting}

When fitting experimental data we vary the parameters of a set of PDF replicas
at the initial scale such that the $\chi^2$ is minimised between the
corresponding theory predictions and a generated pseudodata replica. A set of
PDFs usually refers to a set of seperate continuous functions, one for each
flavour of PDF in a particular basis. In this specific study, fits performed
with \nfit\ parameterise the set of PDFs as a single neural network which takes
as input $x$ and $\ln x$ and returns 8 outputs, one for each flavour in the
fitting basis, multiplied by some preproccessing exponents. The output for a
single flavour $j$ is
\begin{equation}
    NN(x, \ln x)_j * x^{1-\alpha_j} * (1-x)^{\beta_j},
\end{equation}
where each flavour has it's own preproccessing exponents $\alpha$ and $\beta$,
parameters that are varied in these fits, and $NN(x, \ln x)_j$ is the
$j^{\rm th}$ output from the neural network.
When an experiment is included in an NNPDF fit, we take the published
experimental central values and uncertainties (statistical and systematic)
and use these pieces of information to generate the pseudodata.
The pseudodata replica is generated
through Monte Carlo sampling by applying noise to the experimental
central values.
After fitting many sets of PDF replicas (usually of order 100 sets),
each set to an independently generated pseudodata replica, we have an ensemble of
PDF replicas.
The aim of this methodology, is to propagate the various sources of
uncertainty involved with fitting PDFs into the functional PDF space in a faithful
manner. This means that the distribution from which the PDF replicas are drawn
from should be representative of the probability distribution of the true
underlying PDF.

The closure test was introduced alongside NNPDF3.0 and will be described below.
At this stage it's important to note that the closure test may serve 3 purposes:
We can test whether or not the ensemble of PDF replicas does reflect the
probability distribution of the true underlying PDF; we can compare two different
fitting methodologies and use estimators to determine which one performs better;
we can try to understand different elements of our own methodology, such as
the different contributions to the PDF uncertainty. The PDF uncertainty will
be used as short-hand for referring to the distribution of replicas for a given
fit, we consider this the PDF uncertainty because if the distribution
of the true underlying PDF is reflected by the distribution of replicas for
a given fit, then the standard deviation of the replicas in PDF space or the
theory predictions obtained from those replicas in data space represents
the uncertainty of the prediction having performed a fit.

\subsection{Closure test.}

If we consider experimental data which we assume
to be multigaussian then the experimental central values, $\levone$, are given by
\begin{equation}\label{eq:levelonedata}
    \levone_{i} = \law_i + \shift_i,
\end{equation}
where $i$ is the index of the data point.
In other words, the experimental values have been shifted away from the true
values given by nature, $\law$, by some shift, $\shift$. The vector of shifts
is drawn from
the multigaussian $\mathcal{N}(0, \cov)$ where $\cov$ is the experimental
covariance matrix. The fitted pseudodata is obtained by adding Monte Carlo
noise, $\noise^{\repind}$, on top of the experimental central values
\begin{equation}
    \levtwo^{\repind}_{i} = \law_i + \shift_i + \noise^{\repind}_{i},
\end{equation}
where the replica index $k$ refers to each replica having a noise vector drawn
independently from $\mathcal{N}(0, \cov)$. The sampling of pseudodata permits
correlations between individual data points through the covariance matrix,
but not between different replicas or fits.

In a closure test, instead of assuming that the experimental central values are
statistically consistent with an underlying law, as in \eqref{eq:levelonedata},
we use a pre-existing PDF as an underlying law and generate the experimental
central value, by drawing the vector of shifts from the distribution
described by the experimental uncertainties. We refer to the predictions
obtained from the underlying law as level 0 data, $\law$, and the generated
experimental central values as level 1 data, $\levone$. Finally the pseudodata
replicas are generated from the $\levone$ data, identically to a fit to
a fit to experimental data. We refer to the pseudodata replicas as level 2
data, $\levtwo$.

The parameters of each PDF replica are determined by minimising $\chi^2$ between
the predictions of each PDF replica and the corresponding pseudodata replica,
as in a fit to experimental data:
\begin{equation}
    \repchis = \frac{1}{\ndata} \sum_{ij} \diffreptwo_i \invcov{ij} \diffreptwo_j,
\end{equation}
where $\model^{\repind}_i$ is the prediction for $i^{\rm th}$ datapoint, from
the $k^{\rm th}$ set of PDFs. After fitting many replicas, the quality of a fit
is often determined by considering the $\chi^2$ between the experimental central
values and the expectation value of the theory predictions
\begin{equation}\label{eq:centralchi2}
    \chi^2 = \frac{1}{\ndata} \sum_{ij} \diffcentone_i \invcov{ij} \diffcentone_j,
\end{equation}
where $\erep{\cdot}$ denotes the mean value across replicas, so $\erep{g}$ is
the mean of the theory predictions across replicas. This $\chi^2$ is a measure
of the difference between the expectation value of the prediction and the
experimental central values in units of the covariance.

In a fit to experimental data we are limited to assessing the performance of a fit
using this quantity because we don't
have knowledge of the underlying theory predictions, $\law$. What's more, the
assumption that all datapoints across the global dataset used in a fit are
mutually consistent with being generated from a single underlying law from
the distribution defined by the uncertainties might not hold, often
referred to as tension in the data. In particular, our model for generating pseudodata
is based on rather coarse information and despite our best efforts we don't know
if the distribution of our pseudodata truly represents the shape of the distribution
the of the experimental central value, merely that we reproduce the first
two moments of the distribution. Accepting these as unavoidable flaws in any
fit to experimental data, we use the closure test to bypass these and check that
at least in the ideal case where there is no tension in the data, and it is
generated faithfully, that the fitting methodology performs well.

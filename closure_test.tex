\section{Monte Carlo approach to fitting data}
% Generally propagating uncertainty such that the result of the fit contains
% Information on possible outcomes of a fit.
Without making this too much about PDFs want to say
\begin{itemize}
    \item Have some data with some error
    \item use that to make an assumption of the distribution of the data central values
    \item consider the use case of our product - people make new predictions and
    test consistency with input assumptions.
    \item we want to provide a distribution of possible outcomes of a fit based
    taking into account the experiment uncertainty, fitting inefficiency etc.
    \item one way of doing this is to instead provide a sample of that
    distribution in the form of MC replicas, from which somebody can generate
    a sample of new predictions
    \item advantage is that we don't need to be able to parameterise the
    distribution in function space, so we don't have to rely on using a simple
    parameterisation and assuming gaussianity (hessian approach)
    \item does this actually work in theory? Test with simple models, work up
    to actual fit, try and determine potential pitfalls along the way.
    \item the aim of this paper is not to decide whether or NNPDF4.0 is a success
    but to present a set of tools which can determine if the fit is successful
    with some input assumptions being satisfied.
\end{itemize}
The next section I think should be assumptions of the data, which is consistent
throughout. Then we can talk about what is minimised in the fit. Possibly
introduce the vector notation.

\section{Fitted data}
If we consider experimental data which we assume
to be multigaussian then the experimental central values, $\levone$, are given by
\begin{equation}\label{eq:levelonedata}
    \levone_{i} = \law_i + \shift_i,
\end{equation}
where $i$ is the index of the data point.
In other words, the experimental values have been shifted away from the true
values given by nature, $\law$, by some shift, $\shift$. The vector of shifts
is drawn from
the multigaussian $\mathcal{N}(0, \cov)$ where $\cov$ is the experimental
covariance matrix. Equation \eqref{eq:levelonedata} is our interpretation of the
experimental data, which is often provided in different forms. In the case that
the experimental data is a set of central values and a covariance matrix then
this interpretation appears entirely natural.
% we never talk about normalisation uncertainties - should we? Maybe later..

The pseudodata which will be fitted is obtained by adding Monte Carlo
noise, $\noise^{\repind}$, on top of the experimental central values
\begin{equation}\label{eq:leveltwodata1}
    \levtwo^{\repind}_{i} = \law_i + \shift_i + \noise^{\repind}_{i},
\end{equation}
where the replica index $k$ refers to each replica having a noise vector drawn
independently from $\mathcal{N}(0, \cov)$. The sampling of pseudodata permits
correlations between individual data points through the covariance matrix,
however each replica is sampled independently from the others.

Each pseudodata replica gets independently fitted by a replica of the model.
The end product of performing a fit is a sample of model replicas which are
sampled from the distribution of possible models given the data we were provided
with and the assumptions we made about it. Considering the vector of observables
calculated from the model replica $k$
for the fitted data, $\model^{\repind}$, the purpose of the level 2 noise
is to induce a dispersion in the predictions calculated from each model replica.
One of the conditions for a successful fit is that the true underlying law
values $\law$ appears as though it could reasonably have been drawn from the
same distribution as the model replicas. In other words, if the model replicas
were drawn from a multivariate gaussian distribution (with correlations over
the data points) then we would want $\law$ to fall within 1$\sigma$ of the mean
across replicas of the observables for 68\% of the datapoints etc. Testing
whether or not this has happened is clearly impossible with real experimental
data, since we have no knowledge of the true values of nature. However the role
of the closure test is to provide empiricle evidence that provided our assumptions
of the data are valid, our methodology does indeed satisfy the condition set out
above.

\section{Fitting the model parameters}

Each model replica has a set of parameters $\theta^\repind$, the theoretical
predictions from model replica $k$ for datapoint $i$ is
$\model_{i}(\theta^{\repind})$, where we have made it explicit that the observable
values calculated from the model depend on the parameters, and the values of the
parameters are distinct for each model replica. In order to determine the values
which $\theta^{\repind}$ should take, we maximise the likelihood of the parameters
given the data. The data which model replica $k$ "sees" is the corresponding
pseudodata $\levtwo^{\repind}$ and so the likelihood of the parameters given
that data is
\begin{equation}
    \mathcal{L}(\theta^{\repind} \vert \levtwo^{\repind}) = p(\levtwo^{\repind} | \theta^{\repind})
\end{equation}
where $\mathcal{L}$ is the likelihood function, which is equal to the probability
density function, $p$, at fixed value $\levtwo^{\repind}$ given
$\theta^{\repind}$. Given our assumption of how $\levtwo^{\repind}$ was obtained
with two independent shifts drawn from the same multivariate gaussian
$\mathcal{N}(0, \cov)$, the probability density function at
$\levtwo^{\repind}$ is, up to normalisation,
\begin{equation}
    p(\levtwo^{\repind} | \theta^{\repind}) \sim
    \exp \left( - \frac{1}{2} \sum_{ij} \diffreptwo_i {(2\cov)}^{-1}_{ij} \diffreptwo_j \right)
\end{equation}
which is the standard expression for a multigaussian with vector of means
$\model(\theta^{\repind})$ and covariance $2C$, the covariance is $2C$ because
when summing two independent multivariate variables of the same dimension, the
sum is still normal with means and covariances summed. In otherwords we could
rewrite \eqref{eq:leveltwodata1} as
\begin{equation}
    \levtwo^{\repind}_{i} = \law_i + \delta^{\repind}_{i}
\end{equation}
where $\delta$ is drawn from the multigaussian $\mathcal{N}(0, 2\cov)$. The
index of $\delta^{\repind}_{i}$ is very confusing because it only explicitly
sets dependence on the replica, and it's clear that the mean and covariance
across replicas of $\levtwo^{\repind}_{i}$, with fixed $\shift$ are
$\levone$ and $\cov$ respectively. It would make more sense to write the
dependence of $\levtwo$ explicitly
\begin{equation}
    \levtwo(\noise^{\repind}, \shift, \law) = \law + \delta(\noise^{\repind}, \shift)
\end{equation}
where $\delta = \noise^{\repind} + \shift$ is still drawn from
$\mathcal{N}(0, 2\cov)$. When fitting a single replica we should imagine that
we do not know the exact value of $\noise^{\repind}$ and disregard the fact
that we are fitting other replicas and simply try to find the set of parameters
$\model(\theta^{\repind})$ which maximises the likelihood of giving that data
when $\delta ~ \mathcal{N}(0, 2\cov)$ is applied. Whilst we are unable to
bootstrap over $\shift$ because the experimental central values are fixed
to the value which are published, will still should account for the fact that
they are a stochastic variable in our model. If we were able to rerun all of
the experiments from scratch then we would also be able to bootstrap over
$\shift$, however this is clearly impossible with real data, but will be
revisited when talking about the empirical closure test.

In practice we actually maximise the log-likelihood,
\begin{equation}
    l(\theta^{\repind} \vert \levtwo^{\repind}) = \ln \left[
        \mathcal{L}(\theta^{\repind} \vert \levtwo^{\repind}) \right],
\end{equation}
because $\ln$ is a monotonic function, so $l$ has a maximum at the same value
of $\theta^{\repind}$. Furthermore, maximising the log-likelihood is the same
as minimising the $\chi^2$ between $\levtwo(\noise^{\repind}, \shift, \law)$
and $\model(\theta^{\repind})$ defined as
\begin{equation}
    \chi^2(\model(\theta^{\repind}); \levtwo(\noise^{\repind}, \shift, \law))
    = \sum_{ij} \diffreptwo_i {(2\cov)}^{-1}_{ij} \diffreptwo_j.
\end{equation}
To be very precise, since
we are interested in finding the minimimum with respespected to the parameters,
or the point at which
$\frac{\partial}{\partial \theta^{\repind}}
\chi^2(\model(\theta^{\repind}); \levtwo(\noise^{\repind}, \shift, \law)) = 0$
it doesn't matter in reality whether we minimise
$\chi^2(\model(\theta^{\repind}); \levtwo(\noise^{\repind}, \shift, \law))$ or
any scalar multiple of that. Also, since ${(2\cov)}^{-1} = \frac{1}{2}\invcov{}$
then we actually minimise
\begin{equation}
    \repchis = \frac{1}{\ndata} \sum_{ij} \diffreptwo_i \invcov{ij} \diffreptwo_j.
\end{equation}
The parameters obtained from minimising this function are the parameters which
maximise the likelihood of
$\mathcal{L}(\theta^{\repind} \vert \levtwo^{\repind})$.
\section{Polynomial model}
Here describe the definition of poly toy model and how to fit
\section{Neural Network Parton distribution functions}
% Should this section go later? I think so.
When fitting experimental data we vary the parameters of a set of PDF replicas
at the initial scale such that the $\chi^2$ is minimised between the
corresponding theory predictions and a generated pseudodata replica. A set of
PDFs usually refers to a set of seperate continuous functions, one for each
flavour of PDF in a particular basis. In this specific study, fits performed
with \nfit\ parameterise the set of PDFs as a single neural network which takes
as input $x$ and $\ln x$ and returns 8 outputs, one for each flavour in the
fitting basis, multiplied by some preproccessing exponents. The output for a
single flavour $j$ is
\begin{equation}
    NN(x, \ln x)_j * x^{1-\alpha_j} * (1-x)^{\beta_j},
\end{equation}
where each flavour has it's own preproccessing exponents $\alpha$ and $\beta$,
parameters that are varied in these fits, and $NN(x, \ln x)_j$ is the
$j^{\rm th}$ output from the neural network.
When an experiment is included in an NNPDF fit, we take the published
experimental central values and uncertainties (statistical and systematic)
and use these pieces of information to generate the pseudodata.
The pseudodata replica is generated
through Monte Carlo sampling by applying noise to the experimental
central values.
After fitting many sets of PDF replicas (usually of order 100 sets),
each set to an independently generated pseudodata replica, we have an ensemble of
PDF replicas.
The aim of this methodology, is to propagate the various sources of
uncertainty involved with fitting PDFs into the functional PDF space in a faithful
manner. This means that the distribution from which the PDF replicas are drawn
from should be representative of the probability distribution of the true
underlying PDF.

The closure test was introduced alongside NNPDF3.0 and will be described below.
At this stage it's important to note that the closure test may serve 3 purposes:
We can test whether or not the ensemble of PDF replicas does reflect the
probability distribution of the true underlying PDF; we can compare two different
fitting methodologies and use estimators to determine which one performs better;
we can try to understand different elements of our own methodology, such as
the different contributions to the PDF uncertainty. The PDF uncertainty will
be used as short-hand for referring to the distribution of replicas for a given
fit, we consider this the PDF uncertainty because if the distribution
of the true underlying PDF is reflected by the distribution of replicas for
a given fit, then the standard deviation of the replicas in PDF space or the
theory predictions obtained from those replicas in data space represents
the uncertainty of the prediction having performed a fit.

\subsection{Closure test}

In a closure test, instead of assuming that the experimental central values are
statistically consistent with an underlying law, as in \eqref{eq:levelonedata},
we use a pre-existing PDF as an underlying law and generate the experimental
central value, by drawing the vector of shifts from the distribution
described by the experimental uncertainties. We refer to the predictions
obtained from the underlying law as level 0 data, $\law$, and the generated
experimental central values as level 1 data, $\levone$. Finally the pseudodata
replicas are generated from the $\levone$ data, identically to a fit to
a fit to experimental data. We refer to the pseudodata replicas as level 2
data, $\levtwo$.

The parameters of each PDF replica are determined by minimising $\chi^2$ between
the predictions of each PDF replica and the corresponding pseudodata replica,
as in a fit to experimental data:
\begin{equation}
    \repchis = \frac{1}{\ndata} \sum_{ij} \diffreptwo_i \invcov{ij} \diffreptwo_j,
\end{equation}
where $\model^{\repind}_i$ is the prediction for $i^{\rm th}$ datapoint, from
the $k^{\rm th}$ set of PDFs. After fitting many replicas, the quality of a fit
is often determined by considering the $\chi^2$ between the experimental central
values and the expectation value of the theory predictions
\begin{equation}\label{eq:centralchi2}
    \chi^2 = \frac{1}{\ndata} \sum_{ij} \diffcentone_i \invcov{ij} \diffcentone_j,
\end{equation}
where $\erep{\cdot}$ denotes the mean value across replicas, so $\erep{g}$ is
the mean of the theory predictions across replicas. This $\chi^2$ is a measure
of the difference between the expectation value of the prediction and the
experimental central values in units of the covariance.

In a fit to experimental data we are limited to assessing the performance of a fit
using this quantity because we don't
have knowledge of the underlying theory predictions, $\law$. What's more, the
assumption that all datapoints across the global dataset used in a fit are
mutually consistent with being generated from a single underlying law from
the distribution defined by the uncertainties might not hold, often
referred to as tension in the data. In particular, our model for generating pseudodata
is based on rather coarse information and despite our best efforts we don't know
if the distribution of our pseudodata truly represents the shape of the distribution
the of the experimental central value, merely that we reproduce the first
two moments of the distribution. Accepting these as unavoidable flaws in any
fit to experimental data, we use the closure test to bypass these and check that
at least in the ideal case where there is no tension in the data, and it is
generated faithfully, that the fitting methodology performs well.

\section{NNPDF Monte Carlo approach to inverse problems}
\label{sec:closure-test}

In this section we will discuss the NNPDF approach to inverse problems, trying
to make contact explicitly with the formalism laid out in
Sec.~\ref{sec:inverse-problems}. In particular, Eq.~\eqref{eq:PosteriorModel}
gives a formal description of propagating our prior understanding of the data
into model space. In practice, sampling from the posterior distribution is
highly non-trivial.

\subsection{Fitting replicas}
\label{sec:fit-reps}

The approach for generating a sample in model space employed by NNPDF can
broadly be described as fitting model replicas to pseudo-data replicas. As
discussed in Eq.~\eqref{eq:NoisyInverseProblem} the experimental values are
subject to observational noise. If we assume this observational noise to be
multigaussian then the experimental central values, $\obspriorcent$, are given
explicitly by
\begin{equation}
    \label{eq:levelonedata}
    \obspriorcent = \law + \obsnoise,
\end{equation}
where $\law$ is the vector of {\em true} observable values and the obervational
noise is drawn from a Gaussian centered on zero such as in
Eq.~\ref{eq:RhoGauss}, \ie\ $\obsnoise \sim \mathcal{N}(0, \obspriorcov)$ where
$\obspriorcov$ is the experimental covariance matrix. In
Eq.~\eqref{eq:levelonedata}, each basis vector corresponds to a separate data
point, and the vector of shifts $\obsnoise$ permits correlations between data
points according to the covariance matrix provided by the experiments. Given the
data, the NNPDF approach is to compute a MAP estimator similar to that discussed
in the previous section, \ie\ finding the model that minimises the $\chi^2$ to
the data. The key difference between the NNPDF approach and the classical MAP
estimator is instead of fitting the observational data given by
Eq.~\ref{eq:levelonedata}, an ensemble of model replicas are fitted each to an
independently sampled instance of pseudo-data, which is generated by augmenting
$\obspriorcent$ with some noise, $\noise^{\repind}$,
\begin{equation}
    \label{eq:leveltwodata1}
    \pseudodat^{\repind} = \obspriorcent + \noise^{\repind}
    = \law + \obsnoise + \noise^{\repind},
\end{equation}
where $k$ is the replica index and each instance of the noise, $\noise$, is drawn
independently from the same Gaussian from which the observational noise is
drawn from, \ie\ $\noise \sim \mathcal{N}(0, \obspriorcov)$.

The parameters for each model replica maximise the likelihood evaluated on the
corresponding pseudo-data. This is a special case of MAP estimation, described
in Eq.~\eqref{eq:MAP}, where the model prior is uniform. Another way of viewing
this is to take $\modelpriorcov^{-1} \to 0$ in Eq.~\eqref{eq:MAP}, as was done
to obtain the result in Eq.~\ref{eq:NoPriorLinModel}. Either way,
there is no prior information about the model. In this framework, the
parameterisation of the model is fixed, so the model space is the finite space
of parameters $\modelvec \in \real^{\nmodel}$. Furthermore, we actually find the
parameters which minimise the $\chi^2$ between the predictions from the model
and the corresponding pseudo-data $\pseudodat^{\repind}$
\begin{equation}\label{eq:NNPDFLikelihood}
    \begin{split}
        \modelvecrep &= \arg\min_{\modelvec^{\repind}} \repchis \\
        &= \arg\min_{\modelvec^{\repind}} \sum_{ij}
        \left( \fwdobsop(\modelvec^{\repind}) - \pseudodat^{\repind} \right)^T
        \obspriorcov^{-1}
        \left( \fwdobsop(\modelvec^{\repind}) - \pseudodat^{\repind} \right) \, ,
    \end{split}
\end{equation}
where, as usual, minimising the $\chi^2$ is equivalent to maximising the
likelihood, $\likelihood$, since $\chi^2 \equiv -\log{\likelihood}$.
% The uncertainty on the MAP estimator is computed by bootstrapping the
% data, \ie\ by generating an ensemble of pseudo-data, called replicas, that
% fluctuate around $\obspriorcent$. By fitting each replica, we obtain an ensemble
% of models whose distribution is representative of the fluctuations of the MAP
% estimator. The fitted pseudo-data is generated by augmenting the data with some
% noise, $\noise^{\repind}$,

As a final note: since we do not include the model prior, overall normalisations
can be omitted in Eq.~\ref{eq:NNPDFLikelihood}. It is clear however that if we
were including a model prior in our MAP, it is important that the relative
normalisations between the likelihood function and the model prior are
self-consistent.

\subsection{Fluctuations of fitted values}
\label{sec:fluct-fit-values}

It is not immediately obvious that our MC methodology, maximising the likelihood
on an ensemble of pseudo-data replicas, should guarantee that the model replicas
are indeed sampled from the posterior distribution of parameters given data as
described \eg\ in Eq.~\ref{eq:PosteriorModel}. In order to investigate this
issue, we will again consider a model, whose predictions are linear in the model
parameters, where the posterior distribution of model parameters can be written
explicitly. A practical example, which can
elucidate the following arguments would be a polynomial model. Then $\modelvec$ is a
vector of $\nmodel$ polynomial coefficients and $\linmap$ is the Vandermonde matrix
\begin{equation}
    \linmap =
    \begin{bmatrix}
        1  & x_1 & \ldots& x_1^{\nmodel-1} \\ 
        1  & x_2 & \ldots& x_2^{\nmodel-1} \\ 
        \vdots  & \vdots & \vdots& \vdots \\ 
        1  & x_{\ndata} & \ldots & x_{\ndata}^{\nmodel-1} 
    \end{bmatrix}.
\end{equation}
In this case the forward map yields
\begin{equation}
    \label{eq:PolyMod}
    y_i = \sum_{j=0}^{\nmodel-1} u_j x_i^j\, , 
\end{equation}
where $i=1,\ldots,\ndata$. The arguments here are not restricted to polynomials,
however, and apply to any model whose forward map can be expressed as
Eq.~\eqref{eq:MatrixG}, for example a linear shallow approximation of neural
networks \cite{ADVANI2020428}. In order to get an exact analytical solution for
the linear model, we additionally require $\linmap$ to have linearly independent
rows, and therefore $\linmap \obspriorcov \linmap^T$ is invertible. With no
prior information on the model, the posterior distribution of model parameters
is a Gaussian with mean and covariance given by Eqs.~\ref{eq:NoPriorLinModel}
and \ref{eq:NoPriorLinModelCov}.

If instead we deploy the NNPDF Monte Carlo method to fitting model replicas,
then in the case under study $\arg\min_{\modelvec^{\repind}} \repchis$ is found
analytically when the derivative of $\repchis$ with respect to the model
parameters is zero, i.e.
\begin{equation}
    \begin{split}
        \label{eq:MAPEstLinModel}
        \modelvecrep &= (\linmap^T \obspriorcov^{-1} \linmap)^{-1}
        \left(
            \linmap^T \obspriorcov^{-1} \obspriorcent +
            \linmap^T \obspriorcov^{-1} \noise^{\repind}
        \right) \, .
    \end{split}
\end{equation}
Eq.~\ref{eq:MAPEstLinModel} shows that $\modelvec_*$ is a linear
combination of the Gaussian variables $\noise$, and therefore is
also a Gaussian variable. Its
probability density is then completely specified by the average and
covariance of $\modelvec_*$, which can be calculated explicitly, given that the
probability density for $\noise$ is known:
\begin{align}
    \emodel{\modelvec_*} &=
    \modelpostcent = (\linmap^T \obspriorcov^{-1} \linmap)^{-1} \linmap^T
    \obspriorcov^{-1} \obspriorcent \\
    {\rm cov}(\modelvec_*) &= \modelpostcov = (\linmap^T \obspriorcov^{-1} \linmap)^{-1} \, .
\end{align}
In this way, we can show explicitly that under the assumptions specified above,
$\modelvec_* \sim \mathcal{N}(\modelpostcent, \, \modelpostcov)$.
In other words, when the model predictions are linear in the model parameters,
the NNPDF MC method is shown to produce a sample of models from the posterior
distribution of model parameters given the data.

\subsection{Closure test}
\label{sec:closure-test-intro}

The concept of the closure test, which was first introduced in
Ref.~\cite{nnpdf30}, is to construct artificial data by using a known
pre-existing function to generate the {\em true} observable values, $\law$. One
way of achieving this is by choosing $\lawmodel$ such that $\law =
\fwdobsop(\lawmodel)$. Then the experimental central values are artificially
generated according to Eq.~\ref{eq:levelonedata}, except the observational noise
is pseudo-randomly generated from the assumed distribution.

In \cite{nnpdf30}, $\law$ is referred to as level 0 (L0) data and
$\obspriorcent$ is referred to as level 1 (L1) data. Finally, if we use the
NNPDF MC method to fit artificially generated closure data, the pseudo-data
replicas that are fitted by the model replicas are referred to as level 2 (L2)
data.

In a closure test, the assumed prior of the data is fully consistent with the
particular instance of observed central values, $\obspriorcent$, by construction.
In the original closure test in NNDPF3.0 there was also no
modelisation uncertainty, the true observable values were assumed to be obtained
by applying the forward map $\fwdobsop$ to a vector in model space $\lawmodel$.
It is worth noting that the assumption of zero modelisation uncertainties is
quite strong and likely unjustified in many areas of physics. In the context of
fitting parton distribution functions there are potentially missing higher order
uncertainties (MHOUs) from using fixed order perturbative calculations as part
of the forward map. MHOUs have been included in parton distribution fits
\cite{AbdulKhalek:2019ihb} and in the future these should be included in the
closure test, however this is beyond the scope of the study presented here,
since MHOUs are still not included in the NNPDF methodology by default. In the
results presented in the rest of this paper we do include nuclear and deuteron
uncertainties, as presented in \cite{Ball:2018twp, Ball:2020xqw}, since they are
to be included in NNPDF fits by default. Extensive details for including
theoretical uncertainties, modelled as theoretical covariance matrices can be
found in those references. For the purpose of this study the modelisation
uncertainty is absorbed into the prior of the data, since
\begin{equation}
    \obspriorcent = \fwdobsop(\modelvec) + \obsnoise + \delta
\end{equation}
where $\delta \sim \mathcal{N}(0, \cov^{\rm theory})$. But since the
modelisation uncertainty is independent of the data uncertainty then from the
point of view of a closure test we can absorb $\delta$ into $\obsnoise$ by
modifying the data prior: $\obsnoise \sim \mathcal{N}(0, C + C^{\rm theory})$,
we must also update the likelihood of the data given the model to use the total
covariance $(C + C^{\rm theory})$. From now onwards we will omit $C^{\rm
theory}$ because it is implicit that we always sample and fit data using the
total covariance matrix which includes any modelisation uncertainty we currently
take into account as part of our methodology.

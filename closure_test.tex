\section{Data and Fitting}

When fitting experimental data we vary the parameters of a set of PDF replicas
at the initial scale such that the $\chi^2$ is minimised between the
corresponding theory predictions and a generated pseudodata replica. A set of
PDFs usually refers to a set of seperate continuous functions, one for each
flavour of PDF in a particular basis. In this specific study, fits performed
with \nfit\ parameterise the set of PDFs as a single neural network which takes
as input $x$ and $\ln x$ and returns 8 outputs, one for each flavour in the
fitting basis, multiplied by some preproccessing exponents. The output for a
single flavour $j$ is
\begin{equation}
    NN(x, \ln x)_j * x^{1-\alpha_j} * (1-x)^{\beta_j},
\end{equation}
where each flavour has it's own preproccessing exponents $\alpha$ and $\beta$,
parameters that are varied in these fits, and $NN(x, \ln x)_j$ is the
$j^{\rm th}$ output from the neural network. The pseudodata replica is generated
through Monte Carlo sampling according to the experimental uncertainty, by
applying noise to the experimental
central values. After fitting many sets of PDF replicas (usually of order 100 sets),
each set to an independently generated pseudodata replica, we have an ensemble of
PDF replicas which is a sample of the probability distribution of the PDF given
the data. The aim of this methodology, is to propagate the various sources of
uncertainty involved with fitting PDFs into the functional PDF space.

If we consider experimental data which we assume
to be multigaussian then the experimental central values, $\levone$, are given by
\begin{equation}
    \levone_{i} = \law_i + \shift_i,
\end{equation}
where $i$ is the index of the data point.
In other words, the experimental values have been shifted away from the true
values given by nature, $\law$, by some shift, $\shift$. The vector of shifts
is drawn from
the multigaussian $\mathcal{N}(0, \cov)$ where $\cov$ is the experimental
covariance matrix. The fitted pseudodata is obtained by adding Monte Carlo
noise, $\noise^{\repind}$, on top of the experimental central values
\begin{equation}
    \levtwo^{\repind}_{i} = \law_i + \shift_i + \noise^{\repind}_{i},
\end{equation}
where the replica index $k$ refers to each replica having a noise vector drawn
independently from $\mathcal{N}(0, \cov)$. The sampling of pseudodata permits
correlations between individual data points through the covariance matrix,
but not between different replicas or fits.

The $\chi^2$ which is minimised for replica $k$ is then given by
\begin{equation}
    \repchis = \frac{1}{\ndata} \sum_{ij} \diffreptwo_i \invcov{ij} \diffreptwo_j,
\end{equation}
where $\model^{\repind}_i$ is the prediction for $i^{\rm th}$ datapoint, from
the $k^{\rm th}$ set of PDFs. After fitting many replicas, the quality of a fit
is often determined by considering the $\chi^2$ between the experimental central
values and the expectation value of the theory predictions
\begin{equation}\label{eq:centralchi2}
    \chi^2 = \frac{1}{\ndata} \sum_{ij} \diffcentone_i \invcov{ij} \diffcentone_j,
\end{equation}
where $\erep{\cdot}$ denotes the mean value across replicas, so $\erep{g}$ is
the mean of the theory predictions across replicas. This $\chi^2$ is a measure
of the difference between the expectation value of the prediction and the
experimental central values in units of the covariance.

In a fit to experimental data we are limited to this quantity because we don't
have knowledge of the underlying theory predictions of nature $\law$. In a
closure test we use a pre-existing PDF as proxy for $\law$ and then
generate both $\shift$ and $\noise$ from $\mathcal{N}(0, \cov)$, emulating the
different levels of data. The underlying theory predictions are referred to as
level zero data. We refer to the shifted central values as level one data, the
underlying law plus a level one shift. The pseudodata which a given replica fits
is then referred to as level two data.

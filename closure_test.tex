\section{NNPDF Monte Carlo approach to inverse problems}
\label{sec:closure-test}

In this section we will discuss the NNPDF approach to inverse problems, trying
to make contact explicitly with the formalism laid out in
Sec.~\ref{sec:inverse-problems}. In particular, Eq.~\eqref{eq:PosteriorModel}
gives a formal description of propagating our prior understanding of the data
into model space. In practice, sampling from the posterior distribution is
highly non-trivial.

The approach for generating a sample in model space employed by NNPDF can
broadly be described as fitting model replicas to pseudo-data replicas. As
discussed in Eq.~\eqref{eq:NoisyInverseProblem} the experimental values are
subject to observational noise. If we assume this observational noise to be
multigaussian then the experimental central values, $\vv{\levone}$, are given
explicitly by
\begin{equation}
    \label{eq:levelonedata}
    \vv{\levone} = \vv{\law} + \vv{\shift},
\end{equation}
with, $\vv{\shift} \sim \mathcal{N}(0, \cov)$ where $\cov$ is the experimental
covariance matrix. In Eq.~\eqref{eq:levelonedata}, each basis vector corresponds
to a separate data point, and the vector of shifts $\vv{\shift}$ permits
correlations between data points according to the covariance matrix provided by
the experiments. 

The fitted pseudo-data is generated by augmenting the data with some
noise, $\noise^{\repind}$,
\begin{equation}
    \label{eq:leveltwodata1}
    \vv{\levtwo}(\vv{\law},\,\vv{\shift},\,\vv{\noise}^{\repind})
    = \vv{\law} + \vv{\shift} + \vv{\noise}^{\repind},
\end{equation}
where the replica index $k$ refers to each replica having a noise vector drawn
independently from $\vv{\noise} \sim \mathcal{N}(0, \cov)$.

The parameters for each model replica maximise the likelihood of getting the
corresponding pseudo-data replicas from the model. This is a special case of MAP
estimation, described in Eq.~\eqref{eq:MAP}, where the model prior is uniform -
in other words there is no prior information in model space. In this framework,
the parameterisation of the model is fixed, so the model space, is the space of
parameters $u \in R^{\nmodel}$. Furthermore, we actually find the parameters
which minimise the $\chi^2$ between the predictions from the model and the
corresponding pseudo-data $\levtwo^{\repind}$
\begin{equation}
    \begin{split}
        u^{\repind *} &= \arg\min_{u^{\repind}} \repchis \\
        &= \arg\min_{u^{\repind}} \sum_{ij} \diffreptwo_i \invcov{ij} \diffreptwo_j\, ,
    \end{split}
    % \exp \left( - \frac{1}{2} \sum_{ij} \diffreptwo_i {(2\cov)}^{-1}_{ij} \diffreptwo_j \right)
\end{equation}
which is equivalent to maximising the likelihood, $\likelihood$, since
$\chi^2 \equiv -\log{\likelihood}$.

It is not immediately obvious that our MC methodology, maximising the likelihood
on an ensemble of pseudo-data replicas, should guarantee that the model replicas
are indeed sampled from the posterior distribution of parameters given data as
described \eg\ in Eq.~\ref{eq:PosteriorModel}. In order to investigate this
issue, we will again consider a model, whose predictions are linear in the model
parameters, where the posterior distribution of model parameters can be written
explicitly. The forward map is given by
\begin{equation}\label{eq:LinForwardMap}
    \mathcal G(u) = X u
\end{equation}
where $X$ is some $\ndata\times\nmodel$ matrix. A practical example, which can
elucidate the following arguments would be a polynomial model. Then $u$ is a
vector of $\nmodel+1$ polynomial coefficients and $X$ is the Vandermonde matrix
\begin{equation}
    X =
    \begin{bmatrix}
        1  & x_1 & \ldots& x_1^{\nmodel} \\ 
        1  & x_2 & \ldots& x_2^{\nmodel} \\ 
        \vdots  & \vdots & \vdots& \vdots \\ 
        1  & x_{\ndata} & \ldots & x_{\ndata}^{\nmodel} 
    \end{bmatrix}.
\end{equation}
In this case the forward map yields
\begin{equation}
    \label{eq:PolyMod}
    y_I = \sum_{k=0}^{\nmodel} u_k x_I^k\, , 
\end{equation}
where $I=1,\ldots,\ndata$. The arguments here are not restricted to polynomials,
however, and apply to any model whose forward map can be expressed as
Eq.~\eqref{eq:LinForwardMap}, for example linear shallow approximation of neural
networks \cite{ADVANI2020428}. If the prior distribution of model parameters is
uniform then the posterior distribution of model parameters given the data is
\begin{equation}
    \label{eq:PosteriorMultiGaussModel}
    \begin{split}
        p(u | z) &\propto
        \exp \left( -\frac{1}{2} (Xu - z)^T \invcov{} (Xu - z)\right) \\
        &= \exp \left( -\frac{1}{2} (u - X^+z)^T X^T\invcov{}X (u - X^+z)\right)\, ,
    \end{split}
\end{equation}
where in the second line we are assuming that $X$ has linearly independent rows,
and therefore $X X^\dagger$ is invertible. Under these hypotheses $X^+$ is a
right inverse and can be computed as
% can we restrict X to be real and just take transpose here?
\begin{equation}
    \label{eq:RightInverse}
    X^+ = X^\dagger \left(X X^\dagger\right)^{-1}\, .
\end{equation}
Eq.~\ref{eq:PosteriorMultiGaussModel} exposes that the posterior distribution of
the model parameters is multigaussian, with mean $\bar{u} = X^+z$ and covariance
$(X^T\invcov{}X)^+ = X^+ \cov (X^T)^+$. If instead we deploy the NNPDF Monte
Carlo method to fitting model replicas, then in the case under study
$\arg\min_{u^{\repind}} \repchis$ is found analytically when the derivative of
$\repchis$ with respect to the model parameters is zero, i.e.
\begin{equation}
    \label{eq:MAPEstLinModel}
    \begin{split}
        u^{\repind *} &= (X^T\invcov{}X)^{+}
        \left( X^T \invcov{} \vv{\levone} + X^T \invcov{} \vv{\noise}^{\repind} \right), \\
        &= X^+ (\vv{\levone} + \vv{\noise}^{\repind}).
    \end{split}
\end{equation}
Eq.~\ref{eq:MAPEstLinModel} shows that $u^*$ is a linear combination of the
Gaussian variables $\epsilon$, and therefore is also a Gaussian variable. Its
probability density is then completely specified by the average and variance of
$u^*$, which can be calculated explicitly, given that the probability density
for $\epsilon$ is known.  In this way, we can show explicitly that under the
assumptions specified above, $u^{*} \sim \mathcal{N}( X^+z, X^+ \cov (X^T)^+)$.
In other words, when the model predictions are linear in the model parameters,
the NNPDF MC method is shown to produce a sample of models from the posterior
distribution of model parameters given the data.
%TODO: move here the narrow prior?

\subsection{Closure test}

The concept of the closure test, which was first introduced in \cite{nnpdf30},
is to construct artificial data by using a known pre-existing function to
generate the {\em true} observable values, $\vv{\law}$. This is achieved by
choosing $\lawmodel$ such that $\vv{\law} = \mathcal{G}(\lawmodel)$. Then the
experimental central values are artificially generated according to
Eq.~\ref{eq:levelonedata} and our assumption $\vv{\shift} \sim \mathcal{N}(0,
\cov)$. In \cite{nnpdf30}, $\vv{\law}$ is referred to as level 0 (L0) data and
$\vv{\levone}$ is referred to as level 1 (L1) data. Finally, if we use the NNPDF
MC method to fit artificially generated closure data, we denote by $\levtwo$ the
pseudo-data replicas that are fitted by the model replicas. These pseudo-data
replicas are referred to as level 2 (L2) data.

In a closure test, our assumption of the prior distribution of the data is
enforced. In the original closure test in NNDPF3.0 there was also no
modelisation uncertainty, the true observable values were assumed to be obtained
by applying the forward map $\mathcal G$ to a vector in model space $\lawmodel$.

It is worth noting that the assumption of zero modelisation uncertainties is
quite strong and likely unjustified in many areas of physics. In the context of
fitting parton distribution functions there are potentially missing higher
order uncertainties (MHOUs) from
using fixed order perturbative calculations as part of the forward map.
MHOUs have been included in parton distribution fits \cite{AbdulKhalek:2019ihb}
and in the future these should be included in the closure test, however this
is beyond the scope of the study presented here, since MHOUs are still not
included in the NNPDF methodology by default. We do include the nuclear and
deuteron uncertainties, as presented in \cite{Ball:2018twp, Ball:2020xqw},
since they are to be included in NNPDF fits by default. Extensive
details for including theoretical uncertainties, modelled as theoretical covariance
matrices can be found in those references. For the purpose of this study the
modelisation uncertainty is absorbed into the prior of the data, since
\begin{equation}
    \levone = \mathcal{G}(u) + \shift + \delta
\end{equation}
where $\delta \sim \mathcal{N}(0, C^{\rm theory})$. But since the
modelisation uncertainty is independent of the data uncertainty then from the
point of view of a closure test we can absorb $\delta$ into $\shift$ by
modifying the data prior: $\shift \sim \mathcal{N}(0, C + C^{\rm theory})$,
we must
also update the likelihood of the data given the model to use the total
covariance $(C + C^{\rm theory})$. From now onwards we will ommit
$C^{\rm theory}$ because it is implicit that we always sample and fit data
using the total covariance matrix which includes any modelisation uncertainty
we currently take into account as part of our methodology.

To summarise, the closure test presented here enforces the assumed prior of the
data and that our assumed modelisation uncertainty properly accounts
for all sources of modelisation uncertainty.

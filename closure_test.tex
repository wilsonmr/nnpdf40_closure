\section{NNPDF Monte Carlo approach to inverse problems}

In this section we will discuss the NNPDF approach to inverse problems, as laid
out in Sec.~\ref{sec:inverse-problems}. In particular,
Eq.~\eqref{eq:PosteriorModel} gives a formal description of propagating our
prior understanding of the data into model space. In practice, sampling from
the posterior distribution is highly non-trivial.

The approach for generating
a sample in model space employed by NNPDF can broadly be described fitting model
replicas to pseudo-data replicas. As discussed in
Eq.~\eqref{eq:NoisyInverseProblem} the experimental values are subject to
observational noise. If we assume this observational noise
to be multigaussian then the experimental central values,
$\vv{\levone}$, are given explicitly by
\begin{equation}\label{eq:levelonedata}
    \vv{\levone} = \vv{\law} + \vv{\shift},
\end{equation}
with, $\vv{\shift} \sim \mathcal{N}(0, \cov)$ where $\cov$ is the
experimental covariance matrix. In Eq.~\eqref{eq:levelonedata},
each basis vector corresponds to a separate data point, and the vector of
shifts $\vv{\shift}$ permits correlations between data points.
% Equation \eqref{eq:levelonedata} is our interpretation of the
% experimental data, can often be provided in different formats, such as central
% values and a breakdown of systematic uncertainties.
% we never talk about normalisation uncertainties - should we? Maybe later..

The fitted pseudo-data is generated according by
augmenting the data with some noise, $\noise^{\repind}$,
\begin{equation}\label{eq:leveltwodata1}
    \vv{\levtwo}(\vv{\law},\,\vv{\shift},\,\vv{\noise}^{\repind})
    = \vv{\law} + \vv{\shift} + \vv{\noise}^{\repind},
\end{equation}
where the replica index $k$ refers to each replica having a noise vector drawn
independently from $\vv{\noise} \sim \mathcal{N}(0, \cov)$.

The parameters for each model replica maximise the
likelihood of getting the corresponding pseudo-data replicas from the
model. This is a special
case of MAP estimation, described in Eq.~\eqref{eq:MAP}, where the model prior
is uniform - in other words there
is no prior information in model space. In this framework, the parameterisation
of the model is fixed, so the model space, is the space of parameters
$u \in R^{\nmodel}$. Furthermore, we actually find the parameters which
minimise the $\chi^2$ between the
predictions from the model and
the correpsonding pseudo-data $\levtwo^{\repind}$
\begin{equation}
    \begin{split}
        u^{\repind *} &= \arg\min_{u^{\repind}} \repchis \\
        &= \arg\min_{u^{\repind}} \sum_{ij} \diffreptwo_i \invcov{ij} \diffreptwo_j,
    \end{split}
    % \exp \left( - \frac{1}{2} \sum_{ij} \diffreptwo_i {(2\cov)}^{-1}_{ij} \diffreptwo_j \right)
\end{equation}
which is equivalent to maximising the likelihood, $\likelihood$, since
$\chi^2 \equiv -\log{\likelihood}$.

It's not immediately obvious that our
MC methodology, maximising the likelihood on an ensemble of pseudo-data replicas,
should guarantee that the model replicas are indeed sampled from the posterior
distribution of parameters given data. We will again consider a model, whose
predictions are linear in the model parameters,
where the posterior distribution of model parameters can be written explicitly.
The forward map is given by
\begin{equation}
    G(u) = X u
\end{equation}
where $X$ is some matrix. If the model is a polynomial, then $\vv{u}$ is
a vector of polynomial coefficients and $X$ is the Vandermonde matrix. If
the prior distribution of model parameters is uniform then the posterior
distribution of model parameters given the data is
\begin{equation}
    \begin{split}
        p(u | z) &\propto
        \exp \left( -\frac{1}{2} (Xu - z)^T \invcov{} (Xu - z)\right) \\
        &= \exp \left( -\frac{1}{2} (u - X^+z)^T X^T\invcov{}X (u - X^+z)\right),
    \end{split}
\end{equation}
where in the second line we expose that the posterior distribution of the model
parameters is multigaussian, with mean $\bar{u} = X^+z$ and covariance
$(X^T\invcov{}X)^+ = X^+ \cov (X^T)^+$. If instead we deploy the NNPDF Monte Carlo method to fitting
model replicas, then $\arg\min_{u^{\repind}} \repchis$ is found analytically
when the derivative of $chi^2$ with respect to the model parameters is zero, i.e.
\begin{equation}
    \begin{split}
        u^{\repind *} &= (X^T\invcov{}X)^{+}
        \left( X^T \invcov{} \vv{\levone} + X^T \invcov{} \vv{\noise} \right), \\
        &= X^+ (\vv{\levone} + \vv{\noise}).
    \end{split}
\end{equation}
Then it's easy to convince ourselves that
$u^{*} ~ \mathcal{N}( X^+z, X^+ \cov (X^T)^+)$. In other words, when the
model predictions are linear in the model parameters, the NNPDF MC method is
shown to produce a sample of models from the posterior distribution of
model parameters given the data. %TODO: move here the narrow prior?

\subsection{Closure test}

TODO!!!!

In a closure test, instead of assuming that the experimental central values are
statistically consistent with an underlying law, as in \eqref{eq:levelonedata},
we use a pre-existing function to generate predictions which we treat as the
underlying law, $\vv{\law}$. We then generate the experimental
central value, by drawing the vector of shifts consistently with our assumption
that $\vv{\shift} \sim \mathcal{N}(0, \cov)$. We refer to predictions obtained
from the input function, $\vv{\law}$, as level 0 data. This are the true values
which we are aiming to reproduce with our models when fitting the pseudodata.
The generated
experimental central values, $\vv{\levone}$, are referred to as level 1 data.
Finally the pseudodata replicas are generated from the $\levone$ data,
analogously to a fit to experimental data. We refer to the pseudodata replicas,
$\vv{\levtwo}$, as level 2 data.
% This needs rewording but emphasise that there are two assumptions which we enforce.

% Get rid of the stuff below.
Whilst the parameters of the model replicas are determined by minimising
the loss function given by equation \eqref{eq:replicaloss}, the overall fit
quality is determined by considering the $\chi^2$ between the experimental
central values and the expectation value of the theory predictions
\begin{equation}\label{eq:centralchi2}
    \chi^2 = \frac{1}{\ndata} {\vecdiffcentone}^{T} \invcov{} \vecdiffcentone,
\end{equation}
where $\erep{\cdot}$ denotes the mean value across replicas, so $\erep{g}$ is
the mean of the theory predictions across replicas. This $\chi^2$ is a measure
of the difference between the expectation value of the prediction and the
experimental central values in units of the covariance. This function is clearly
related to the probability of getting the experimental central values, given
the central model predictions, $p(\levone \vert \erep{\model})$ in anaology to
how the log-likelihood, $\logll$, is related to $p(\levtwo \vert \model)$. In
the limit of infinte data we expect $\chi^2 \rightarrow 1$. In a fit to
experimental data we are limited to assessing the performance of a fit
using this quantity because we don't have knowledge of the underlying
theory predictions, $\vv{\law}$. What's more, we are relying on a number of 
assumptions such as the experimental central values being distributed
according self consistently as a multigaussian centered on the underlying law
with covaraince given by the experimental covariance matrix. The aim of the
closure test is to be able to determine that if our assumptions are true, then
does the methodology at least pass some criteria in the ideal case. Determining
whether this conclusions from the closure test hold with real world data is
outside of the scope of this paper, and essentially a large part of the discussion
in the main NNPDF papers. Criteria which we want to be able to test with the closure
test are: Whether the distribution of replicas for each fit is
consistent with the distribution of fits about the underlying law, and
whether the methodology avoids overfitting the fluctuations in the data. One we are
confident that a methodology satisfies both of those criteria, we would also
like to have some performance indicators which tell us out of a number of proposed
candidate methodologies which produces the best results. Some statistical
estimators will be presented which aim to answer these questions and then the
estimators will be discussed within the context of several models. Finally we
will look at the feasibility and usefulness of these statistical estimators in
the context of fitting parton distributions with neural networks within the
NNPDF framework.

\section{Estimators}
Since we have access to the underlying predictions in a closure test we can
define some new estimators which can be used to determine how well a fit performs

\paragraph{NOTE}{
    do I need to derive bias and variance explicitly or can we just
    reference it?
}

\subsection{Bias}

Defined as the difference between the central value of the replica predictions
and the underlying predictions in units of the covariance, given by
\begin{equation}
    \bias = \frac{1}{\ndata} \sum_{ij} \diffcentunder_i \invcov{ij} \diffcentunder_j.
\end{equation}
It is desirable for a fit to have a smaller bias because that indicates
that the fit is reproducing the underlying predictions well.

\subsection{Variance}

The variance of the replica predictions in units of the covariance
\begin{equation}
    \var = \frac{1}{\ndata} \erep{ \sum_{ij} \diffcentrep_{i} \invcov{ij} \diffcentrep_{j}},
\end{equation}
which can be interpreted as the uncertainty of the PDF in the space of data.

\subsection{Multiple Closure Fits}

As well as being able to calculate the bias estimator in a closure test and
see how well the underlying law is reproduced, an advantage to testing the
methodology with closure tests is the ability to generate ensembles of
level one data - or experimental central values. This facilitates testing
whether or not the distribution of replicas for a single fit is representative of the
distribution of central predictions. There are different methods to test if this
is the case each with disadvantages and advantages.

\subsubsection{Bias-Variance Ratio}

If the uncertainty is faithful then we would expect that upon refitting the
difference between the central value of the replica predictions and the underlying
law is represented properly by the uncertainty of a given PDF. If this is true
then if we were to perform multiple fits with different shifts and calculate the
expectation value of the bias across fits, $\eshift{\bias}$, then we should find
that this equals $\eshift{\var}$, or equivalently
\begin{equation}
    \frac{\eshift{\bias}}{\eshift{\var}} = 1.
\end{equation}
We note that this quantity is slightly coarse: we are checking that the mean square
difference between central predictions and underlying law is the same as the
mean square difference between replica predictions and their central values.
It's also worth noting that this quantity is a squared quantity, the square
root of this can be interpreted as how much uncertainty has been over or
underestimated e.g $\sqrt{\frac{\eshift{\bias}}{\eshift{\var}}} = 0.9$ would
mean the uncertainty is over estimated by 10\%.

\subsubsection{Quantile statistis: $\xi_{1\sigma}$}

A closure test estimator which was previously defined in PDF space was $\xi_{1\sigma}$.
We define here an analogous estimator in data space
\begin{equation}
    \xi_{1\sigma} = \frac{1}{\ndata \nfits} \sum_{il}
    I_{[-\sigma_i^{(l)}, \sigma_i^{(l)}]}
    \left( \erep{\model_i}^{(l)} - \law_i \right),
\end{equation}
where $\sigma^{(l)}$ is the standard deviation of the theory predictions
estimated from the replicas of fit $l$. $\xi_{1\sigma}$ aims to measure the same
thing as $\frac{\eshift{\bias}}{\eshift{\var}}$: whether the distribution of
replicas for a given fit represents the distribution of the central predictions
around the underlying predictions. It's useful to define $\xi_{1\sigma}^{i}$ as
the value of $\xi_{1\sigma}$ for an individual data point
such that
\begin{equation}
    \xi_{1\sigma} = \frac{1}{\ndata} \sum_i \xi_{1\sigma}^{i}.
\end{equation}
Using some fairly modest assumptions we can
calculate what the expected value of $\xi_{1\sigma}^i$ will be. If we assume that
the replica distribution is constant across
fits then
\begin{equation}
    \xi_{1\sigma}^{i} = \int_{-\infty}^{\infty} I_{[-\sigma_i, \sigma_i]}
    p(\erep{\model_i} - \law_i)
    {\rm d}(\erep{\model_i} - \law_i),
\end{equation}
the expectation value of the indicator function across fits.
If we then assume that $p(\erep{\model_i} - \law_i)$ is a gaussian centered on zero
with a standard deviation of $\modelstd_i$ then the integral simplifies to
\begin{equation}
    \xi_{1\sigma}^{i} = \erf \left( \frac{\sigma_i}{\modelstd_i \sqrt{2}}\right),
    \label{eq:expectedxi}
\end{equation}
which is the standard result of integrating a gaussian over some finite symmetric
interval. Clearly if the distribution of central predictions about the underlying law
matches the distribution of the replica predicitons around the central predictions
then the expected value of $\xi_{1\sigma}^{i}$ is 0.68. This is consistent with
the assumptions we made, it's the quantile statistics of a gaussian distribution.

One can also look at the variance of the indicator function across fits to
get the variance of $\xi_{1\sigma}^{i}$
\begin{equation}
    \begin{split}
        {\rm var}[\xi_{1\sigma}^{i}] =& \int_{-\infty}^{\infty} I_{[-\sigma_i, \sigma_i]}^2
        p(\erep{\model_i} - \law_i)
    {\rm d}(\erep{\model_i} - \law_i) - \\
    &\left( \int_{-\infty}^{\infty} I_{[-\sigma_i, \sigma_i]}
    p(\erep{\model_i} - \law_i)
    {\rm d}(\erep{\model_i} - \law_i) \right)^2,
    \end{split}
\end{equation}
which can be simplified to
\begin{equation}
    {\rm var}[\xi_{1\sigma}^{i}] =
    \erf \left( \frac{\sigma_i}{\modelstd_i \sqrt{2}}\right) -
    \erf \left( \frac{\sigma_i}{\modelstd_i \sqrt{2}}\right)^2.
\end{equation}
Even in the ideal case that the distributions are the same, we see that the
${\rm var}[\xi_{1\sigma}^{i}] = 0.22$, which is a large spread considering
$\xi_{1\sigma}^{i}$ is bounded between 0 and 1.

When taking the mean across datapoints to obtain $\xi_{1\sigma}$ we note that
getting 0.68 relies on each sampled $\xi_{1\sigma}^{i}$ being statistically
independent. This clearly will not be the case because there can be a big correlation
between datapoints within the same dataset. We can calculate $\xi_{1\sigma}$
in different basis and note that unlike $\chi^2$ and quantities of the form
$v^T M v$, $\xi_{1\sigma}$ is
not basis independent. There is a choice of basis, however it will be useful to
compare the value $\xi_{1\sigma}$ and $\frac{\eshift{\bias}}{\eshift{\var}}$.
Therefore, a natural basis to calculate
$\xi_{1\sigma}$ is the basis which diagonalises the experimental covariance matrix
because bias and variance are both calculated in units of the experimental
% Should I mention this without having tried it?
covariance. Alternatively one could estimate the covariance of the difference
between replicas and central values and then rotate the differences between
the central predictions and the underlying law into the basis which diagonalises
this replica covariance matrix.

\subsection{Out of sample}

We want to calculate the closure test estimators on data which is out of sample
to check uncertainty is faithful on predictions. Can we justify this choice better
other than we get better results out of sample? In toy model it doesn't make a
difference.

\subsection{PDF estimators}

Fill this later, we can calculate the estimators in PDF space.


\section{Inverse Problems}
\label{sec:inverse-problems}

The problem of determining PDFs from a set of experimental data falls under the
general category of {\em inverse problems}, \ie\ the problem of finding the
input to a given model knowing a set of observations, which are often finite and
noisy. In this Section we are going to review the Bayesian formulation of
inverse problems. It is impossible to do justice to vast subject here. Instead
we try to emphasise the aspects that are relevant for quantifying uncertainties
on PDF determinations. 

\subsection{Statement of the problem}
\label{sec:BayesianInverse}

The space of inputs is denoted by $X$, while $R$ denotes the space of responses.
The model is specified by a {\em forward map}
\begin{align}
  \label{eq:ForwardMap}
  G : ~& X \to R \nonumber \\
      & u \mapsto r=G(u) \, ,
\end{align}
which associates a response $r \in R$ to the input $u \in X$, where we assume
that $X$ and $R$ are Banach spaces.~\footnote{Banach spaces are complete normed
vector spaces. We do not need to get into a more detailed discussion here, but
it is important to note that working in Banach spaces allows us to generalise
the results to infinite-dimensional spaces of functions.} As an example we can
think of $u$ as being a Parton Distribution Function, \ie\ a function defined on
the interval $[0,1]$, and $r$ a DIS structure function,
\begin{align}
  \label{eq:DISExample}
  r(x,Q^2) = \int_x^1 \frac{dz}{z}\, C(z,Q^2) u(x/z,Q^2)\, .
\end{align}
Note that in this example the forward map maps one real function into another
real function. Experiments will not have access to the full function $r$ but
only to a subset of $\ndata$ observations. In order to have a formal
mathematical expression that takes into account the fact that we have a finite
number of measurements, we introduce an {\em observation operator}
\begin{align}
  O : ~& R \to Y \nonumber \\
       & r \mapsto y \, ,
\end{align}
where $y \in Y$ is a vector in a finite-dimensional space $Y$ that contains all
the experimental results, \eg\ the value of the structure function for different
values of the kinematics. In general we will assume that $y \in
\mathbb{R}^{\ndata}$, \ie\ we have a finite number $\ndata$ of real experimental
values. The quantity of interest is the composed operator
\begin{align}
  \mathcal{G} : ~& X \to \mathbb{R}^{\ndata} \nonumber \\
                 & \mathcal{G} = O \circ G\, ,
\end{align}
which maps the input $u$ to the set of data. Taking into account the fact that
experimental data are subject to noise, we can write
\begin{align}
  \label{eq:NoisyInverseProblem}
  y = \mathcal{G}(u) + \eta\, ,
\end{align}
where $\eta$ is a random variable defined over $\mathbb{R}^{\ndata}$
with probability density $\rho(\eta)$. We will refer to $\eta$ as the
{\em observational noise}. The inverse problem becomes finding $u$
given $y$. It is often the case that inverse problems are ill-defined
in the sense that the solution may not exist, may not be unique, or
may be unstable under small variations of the problem. 

In solving this problems we are going to adopt a Bayesian point of view; our
prior knowledge about $u$ is encoded in a prior probability measure
$\mu^0$, and we use Bayes' theorem to compute the posterior
probability measure of $u$ given the data $y$, which we denote as
$\mu^y$. We denote the probability densities associated to $\mu^0$ and
$\mu^y$ by $\pi^0$ and $\pi^y$ respectively. Then, using
Eq.~(\ref{eq:NoisyInverseProblem}), we can write the data likelihood,
\ie\ the probability density of $y$ given $u$,
\begin{align}
  \label{eq:YGivenUProbDensity}
  \pi(y|u) = \rho(y-\mathcal G(u))\, ,
\end{align}
and Bayes' theorem yields
\begin{align}
  \label{eq:BayesThmInversePosterior}
  \pi^y(u) = \pi(u|y) \propto \rho(y-\mathcal G(u)) \pi^0(u)\, .
\end{align}

\paragraph{Example}

The concepts that we have introduced so far should sound familiar, and are best
exemplified by considering the case where both the observational noise and the
model prior are Gaussian. We assume that we are given a set of central values
$y_0 \in \mathbb{R}^{\ndata}$ and their covariance matrix $C_D$. Then the {\em
prior} probability density of the observable $y$ is 
\begin{equation}
  \label{eq:PriorData}
  \pi_{D}^0(y|y_0) \propto \exp\left(
    -\frac12 \left| y - y_0 \right|_{C_D}^2
    \right)\, ,
\end{equation}
% \begin{equation}
%   \label{eq:PriorData}
%   \pi_{D}^0(y|y_0) \propto \exp\left(
%     -\frac12 \left| y - \mathcal{G}u_\mathrm{true}-\eta \right|_{C_D}^2
%     \right)\, ,
% \end{equation}
where the suffix $D$ emphasises the fact that this is a probability density in
data space, and the notation explicitly reminds us that this is the probability
density given the central values $y_0$ (and the covariance matrix). Similarly we
can choose a Gaussian distribution for the input model, characterized by a
central value $u_0$ and a covariance $C_M$:
\begin{align}
  \label{eq:PiZeroGauss}
  \pi_{M}^0(u|u_0)  &\propto \exp\left(
              -\frac12 \left| u - u_0 \right|_{C_M}^2
              \right)\, .
\end{align}
Following the convention above, we use a suffix $M$ here to remind the reader
that we are looking at a probability density in the space of models. Note that
in the expressions above we used the norms in $X$ and $\mathbb{R}^{\ndata}$
respectively, and introduced the short-hand notation
\begin{align}
  \left|a\right|_M^2 = \left| M^{-1/2} a\right|^2\, ,
\end{align}
where $a$ denotes a generic element of $X$, $R$ or $\mathbb{R}^{\ndata}$. For
the case where $a \in \mathbb{R}^{\ndata}$, we can use the usual Euclidean norm
and
\begin{align}
  \left| a \right|_M^2 = \sum_{i,j} a_i M_{ij} a_j\, ,
\end{align}
where the indices $i,j$ run from 1 to $\ndata$.  
Up to this point data and models are completely independent, and the joint
distribution is simply the product of $\pi_{D}^0$ and $\pi_{M}^0$. 

The forward map induces a correlation between the input model and the
observables, 
\begin{equation}
  \label{eq:ThetaCorr}
  \theta(y,u|\mathcal{G}) = \delta\left(y - \mathcal{G}(u)\right)\, ,
\end{equation}
where the Dirac delta corresponds to the case where there are no theoretical
uncertainities. Theoretical uncertainties can be introduced by broadening the
distribution of $y$ away from the exact prediction of the forward map, \eg\
using a Gaussian with covariance $C_T$,
\begin{equation}
  \label{eq:TheoryErrors}
  \theta(y,u|\mathcal{G}) = \exp\left(
    -\frac12 
    \left| y - \mathcal{G}(u)
    \right|_{C_T}^2\right)\, .
\end{equation}
Note however that it is not cleat that theoretical errors are normally
distributed. Taking the correlation $\theta(y,u)$ into account, the joint
distribution of $y$ and $u$ is
\begin{align}
  \label{eq:JointYAndU}
  \pi(y,u|y_0,u_0,\mathcal{G}) \propto 
  \pi_{D}^0(y|y_0) \pi_{M}^0(u|u_0) \theta(y,u|\mathcal{G})\, .
\end{align}
We can now marginalize with respect to y, neglecting theory errors, 
\begin{align}
  \label{eq:MarginOne}
  \pi_M(u|y_0,u_0,\mathcal{G}) 
  &\propto \int dy\, \pi_{D}^0(y|y_0) \pi_{M}^0(u|u_0) 
    \theta(y,u|\mathcal{G}) \\
  & \propto \pi_{M}^0(u|u_0)  \int dy\, \pi_{D}^0(y|y_0) 
    \theta(y,u|\mathcal{G}) \\
  & \propto \pi_{M}^0(u|u_0) \pi_{D}^0(\mathcal{G}(u)|y_0)\, .
\end{align}
The log-likelihood in the Gaussian case is simply the $\chi^2$ of the data to
the theory prediction:
\begin{equation}
  \label{eq:LikelyChiSq}
  -\log\pi_D^0(\mathcal{G}(u)|y_0) =  
      \frac12 \left|
      \mathcal{G}(u) - y_0
      \right|_{C_D}^2
    \, .
\end{equation}
In the notation of Eq.~\ref{eq:BayesThmInversePosterior}
\begin{equation}
  \label{eq:IdentifyRho}
  \pi_D^0(\mathcal{G}(u)|y_0) = \rho\left(
    \mathcal{G}(u) - y_0
  \right)\, ,
\end{equation}
where in this case 
\begin{align}
  \label{eq:RhoGauss}
  \rho(\eta) &\propto \exp\left(
               -\frac12 \left|\eta\right|_{C_D}^2
               \right)\, .
\end{align}
The probability density $\pi_M(u|y_0,u_0,\mathcal{G})$ was called $\pi^y(u)$ in
Eq.~\ref{eq:BayesThmInversePosterior}, where the suffix $y$ is a short-hand to
denote the posterior probability in model space, taking into account all the
conditional variables. Hence, for the Gaussian case, Bayes' theorem yields
\begin{align}
  \label{eq:PosteriorModel}
  \pi^y(u) &\propto 
  \exp\left(
  -\frac12 \left| y_0 - \mathcal G(u) \right|_{C_D}^2
  -\frac12 \left| u - u_0 \right|_{C_M}^2
  \right)\\ 
  &= \exp\left[
    - S(u)
  \right]\, .
\end{align}
Note that in the argument of the likelihood function we have the central values
of the data points $y_0$. Eq.~(\ref{eq:PosteriorModel}) is the Bayesian answer
to the inverse problem, our knowledge of the model $u$ is encoded in the
probability measure $\mu^y$, which is fully specified by the density $\pi^y$.
There are several ways to characterise a probability distribution. The NNPDF
approach is focused on the determination of the {\em Maximum A Posteriori (MAP)}
estimator, \ie\ the element $u_* \in X$ that maximises $\pi^y(u)$:
\begin{align}\label{eq:MAP}
  u_* = \arg\min_{u \in X} 
  \left(
  -\frac12 \left| y_0 - \mathcal G(u) \right|_{C_D}^2
  -\frac12 \left| u - u_0 \right|_{C_M}^2
  \right)\, .
\end{align}
For every instance of the data $y_0$, the MAP estimator is computed by
minimising a regulated $\chi^2$, we will refer to this procedure as the {\em
classical fit} of experimental data to a model. Note that in the Bayesian
approach, the regulator appears naturally after having specified carefully all
the assumptions that enter in the prior. In this specific example the regulator
arises from the Gaussian prior for the model input $u$, which is normally
ditributed around a solution $u_0$. The MAP estimator provides the explicit
connection between the Bayesian approach and the classical fit.

\subsection{Comparison with classical fitting}
\label{sec:comp-class-fit}

There are a number of results that make the connection between the two
approaches more quantitative, and therefore more transparent. We are
going to summarise these results here without proofs, referring the
reader to the mathematical literature for the missing details. Working
in the finite-dimensional case, we assume 
\begin{align*}
  u &\in \mathbb{R}^{\nmodel} \, ,\\
  y &\in \mathbb{R}^{\ndata}\, ,
\end{align*}
and we are going to consider in detail two examples from Ref.~\cite{StuartCore}.

\paragraph{Underdetermined system}
The first case that we are going to analyse is the case of a linear system that
is underdetermined by the data. The linear model is completely specified by a
vector of coefficients $g\in \mathbb{R}^{\nmodel}$, while we are going to assume
that we only have one datapoint, \ie\ $\ndata=1$, and hence:
\begin{equation}
  \label{eq:LinearModelEx}
  y = (g^T u) + \eta\, ,
\end{equation}
where $\eta \sim \mathcal{N}(0,\gamma^2)$ is one Gaussian number, whose
probability density is centred at $0$ and has variance $\gamma^2$. For
simplicity we are going to assume that the prior on $u$ is also a
multi-dimensional Gaussian, centred at $0$ with covariance matrix $C_M$. In
this case the posterior distribution can be written as
\begin{equation}
  \label{eq:GaussPostExplicit}
    \pi^y(u) \propto \exp \left[
    -\frac{1}{2\gamma^2} \left|y - (g^T u) \right|^2 - \frac12 \left|
      u
    \right|_{C_M}^2 
    \right]\, ,
\end{equation}
which is still a Gaussian distribution for $u$. The mean and covariance are
respectively
\begin{align}
  m &= \frac{(C_M g) y}{\gamma^2 + (g^T C_M g)}\, , \\
  \Sigma &= C_M - 
  \frac{(C_M g) (C_M g)^T}{\gamma^2 + (g^T C_M g)}\, .
\end{align}
Because the argument of the exponential is a quadratic form, the mean of the
distribution coincides with the MAP estimator. It is instructive to look at
these quantities in the limit of infinitely precise data, \ie\ in the limit
$\gamma\to 0$:
\begin{align}
  m_\star &= 
  \lim_{\gamma\to 0} m
  = \frac{(C_M g) y}{(g^T C_M g)}\, , \\
  \Sigma_\star &= 
  \lim_{\gamma\to 0} \Sigma 
  = C_M - 
  \frac{(C_M g) (C_M g)^T}{(g^T C_M g)}\, .
\end{align}
These values satisfy
\begin{align}
  (g^T m_\star) = y \, , \\
  (\Sigma_\star g) = 0 \, ,
\end{align}
which shows that the mean of the distribution is such that the data point is
exactly reproduced by the model, and that the uncertainty in the direction
defined by $g$ vanishes. It should be noted that the uncertainty in directions
perpendicular to $g$ does not vanish and is determined by a combination of the
prior and the model, \viz\ $C_M$ and $g$ in our example. This is a
particular example of a more general feature: for underdetermined systems the
information from the prior still shapes the probability distribution of the
solution even in the small noise limit.  

\paragraph{Overdetermined system}
We are now going to consider an example of an overdetermined system and discuss
again the case of small observational noise. We consider $\ndata\geq 2$ and
$n=1$, with a linear forward map such that
\begin{equation}
 \label{eq:OverDetForwMap}
 y = g u  + \eta\, ,
\end{equation} 
where $\eta$ is an $\ndata$-dimensional Gaussian variable with a diagonal
covariance $\gamma^2 I$, where $I$ denotes the identity matrix. For simplicity
we are going to assume a Gaussian prior with unit variance for $u$, which yields
for the posterior distribution:
\begin{equation}
  \label{eq:OverDetPost}
  \pi^y(u) \propto 
    \exp\left(
      -\frac{1}{2\gamma^2} \left| y - g(u + \beta u^3)\right|^2
      -\frac12 u^2
    \right)\, .
\end{equation} 
If $\beta=0$ the posterior is Gaussian and we can easily compute its mean and variance: 
\begin{align}
  m &= \frac{(g^T y)}{\gamma^2 + |g|^2} \, , \\
  \sigma^2 &=
    \frac{\gamma^2}{\gamma^2 + |g|^2}\, .
\end{align}
In this case, in the limit of vanishing observational noise, we obtain
\begin{align}
  m_\star &= \frac{(g^T y)}{|g|^2} \, ,\\
  \sigma_\star^2 &= 0\, .
\end{align}
The mean is given by the weighted average of the datapoints, which is also the solution of the $\chi^2$ minimization
\begin{equation}
  m_\star = \arg\min_{u\in\mathbb{R}} \left|y - g u\right|^2\, .
\end{equation}
Note that in this case the variance $\sigma_\star$ vanishes independently of the
prior. In the limit of small noise, the distribution tends to a Dirac delta
around the value of the MAP estimator.  

\subsection{Linear Problems}
\label{eq:LinProbs}

Linear problems in finite-dimensional spaces are characterized by a simple forward law, 
\begin{equation}
  \label{eq:MatrixG}
  y = \mathcal{G} u\, ,
\end{equation}
where $\mathcal{G}$ is a matrix. In this framework one can readily  derive
analytical solutions that are useful to understand the main features of the
Bayesian approach. Assuming that the priors are Gaussian again, the cost
function $S(u)$ is a quadratic function of $u$,
\begin{align}
  \label{eq:CostLinGauss}
  S(u) &= 
  \left(\mathcal{G} u - y_0 \right)^T C_D^{-1} 
  \left(\mathcal{G} u - y_0 \right) + 
  \left( u - u_0 \right)^T C_M^{-1} \left(u - u_0 \right) \\
  &= 
  \left(u - \tilde{u}\right) \tilde{C}_M^{-1}
  \left(u - u_0\right) + \mathrm{const}\, ,
\end{align} 
where
\begin{align}
  \label{eq:PostParams}
  \tilde{C}_M^{-1} &= 
  \left(
    \mathcal{G}^T C_D^{-1} G + C_M^{-1}
  \right)\, , \\
  \tilde{u} &=
  \tilde{C}_M  \left(
    \mathcal{G}^T C_D^{-1} y_0 + C_M^{-1} u_0
  \right)\, .
\end{align}
The case where we have no prior information on the model is recovered by taking
the limit $C_M^{-1} \to 0$, which yields
\begin{align}
  \label{eq:NoPriorLinModel}
  \tilde{C}_M^{-1} &= 
  \left(
    \mathcal{G}^T C_D^{-1} \mathcal{G}
  \right)\, , \\
  \tilde{u} &=
  \tilde{C}_M  \left(
    \mathcal{G}^T C_D^{-1} y_0 
  \right)\, .
\end{align}
The action of $C_D^{-1}$ on the vector of obseved data $y_0$ is best visualised using a spectral decomposition
\begin{equation}
  \label{eq:CDSpecDec}
  C_D^{-1} = \sum_n \frac{1}{\sigma_n^2} P_n\, ,
\end{equation}
where $P_n$ denotes the projector on the $n$-th eigenvector of $C_D$, and
$\sigma_n^2$ is the corresponding eigenvalue. The action of $C_D^{-1}$ is to
perform a weighted average of the components of $y_0$ in the directions of the
eigenvectors of $C_D$.

An explicit expression can also be obtained for the posterior distribution of
data, which can be obtained from the joint distribution by marginalising over
the model input u:
\begin{align}\label{eq:PosteriorDataSpace}
  \pi_D(y|y_0,u_0,\mathcal{G})
  &= \int du\, \pi(y,u|y_0,u_0,\mathcal{G}) \\
  &\propto \exp\left(
    -\frac12 \left(y - \tilde{y}\right)^T \tilde{C}_D^{-1}
    \left(y - \tilde{y}\right)
  \right)\, ,
\end{align}
where
\begin{align}\label{eq:PosteriorDataParams}
  \tilde{y} &= \mathcal{G} \tilde{u}\, , \\
  \tilde{C}_D &= \mathcal{G} \tilde{C}_M \mathcal{G}^T\, .
\end{align}

{\bf we should be able to use this to understand better the distribution of the estimator in data space} 

\paragraph{Non-linear Models}

These linear models may look over-simplified at first sight. In practice, it
turns out that non-linear models can often be linearised around the central
value of the prior distribution, 
\begin{equation}
  \label{eq:LinU0}
  \mathcal{G}(u) = \mathcal{G}(u_0) + G \left(u - u_0\right) + \ldots\, ,
\end{equation}
where 
\begin{equation}
  \label{eq:FirstDerU0}
  G^i_\alpha = \left. \frac{\partial \mathcal{G}^i}{\partial u_\alpha} \right|_{u_0}\, ,
\end{equation}
where we have neglected higher-order terms in the expansion of $\mathcal{G}(u)$.

If these terms are not negligible, another option is to find the MAP estimator,
and then expand the the forward map around it, which yields equations very
similar to the previous ones, with $u_0$ replaced by $u_\mathrm{MAP}$. If the
posterior distribution of $u_\mathrm{MAP}$ is sufficiently peaked around the
central value, then the linear approximation can be sufficiently accurate. 


\subsection{The infinite-dimensional case}
\label{sec:infin-dimens-case}

In the finite-dimensional case, where the probability measures are
specified by their densities with respect to the Lebesgue measure,
Eq.~(\ref{eq:BayesThmInversePosterior}) encodes the fact that $\rho$
is the Radon-Nikodym derivative of the probability measure $\mu^y$
with respect to $\mu_0$, \viz
\begin{align}
  \label{eq:RadonNikodym}
  \frac{d\mu^y}{d\mu_0} (u) \propto \rho(y-\mathcal G(u)) \pi_0(u)\, .
\end{align}
Finally, using the fact that the density $\rho$ is a positive
function, we can rewrite 
\begin{align}
  \label{eq:PotentialDef}
  \rho(y-\mathcal G(u)) = \exp\left(-\Phi(u;y)\right)\, ,
\end{align}
and therefore
\begin{align}
  \label{eq:RadonNikodymTwo}
  \frac{d\mu^y}{d\mu^0} (u) \propto \exp\left(-\Phi(u;y)\right)\, .
\end{align}
In finite-dimensional spaces, the three equations above are just
definitions that do not add much content. Their interest resides in
the fact that the last expression, Eq.~(\ref{eq:RadonNikodymTwo}), can
be properly defined when $X$ is infinite-dimensional.


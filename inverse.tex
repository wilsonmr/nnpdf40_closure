
\section{Inverse Problems}
\label{sec:inverse-problems}

The problem of determining PDFs from a set of experimental data falls under the
general category of {\em inverse problems}, \ie\ the problem of finding the
input to a given model knowing a set of observations, which are often finite and
noisy. In this Section we are going to review the Bayesian formulation of
inverse problems. It is impossible to do justice to vast subject here. Instead
we try to emphasise the aspects that are relevant for quantifying uncertainties
on PDF determinations. 

\subsection{Statement of the problem}
\label{sec:BayesianInverse}

The space of inputs is denoted by $\modelspace$, while $R$ denotes the space of responses.
The model is specified by a {\em forward map}
\begin{align}
  \label{eq:ForwardMap}
  \fwdmapop : ~& \modelspace \to R \nonumber \\
      & \modelvec \mapsto r=\fwdmapop(\modelvec) \, ,
\end{align}
which associates a response $r \in R$ to the input $\modelvec \in \modelspace$, where we assume
that $\modelspace$ and $R$ are Banach spaces.~\footnote{Banach spaces are complete normed
vector spaces. We do not need to get into a more detailed discussion here, but
it is important to note that working in Banach spaces allows us to generalise
the results to infinite-dimensional spaces of functions.} As an example we can
think of $\modelvec$ as being a Parton Distribution Function, \ie\ a function defined on
the interval $[0,1]$, and $r$ a DIS structure function,
\begin{align}
  \label{eq:DISExample}
  r(x,Q^2) = \int_x^1 \frac{dz}{z}\, C(z,Q^2) \modelvec(x/z,Q^2)\, .
\end{align}
Note that in this example the forward map maps one real function into another
real function. Experiments will not have access to the full function $r$ but
only to a subset of $\ndata$ observations. In order to have a formal
mathematical expression that takes into account the fact that we have a finite
number of measurements, we introduce an {\em observation operator}
\begin{align}
  O : ~& R \to Y \nonumber \\
       & r \mapsto \obs \, ,
\end{align}
where $\obs \in Y$ is a vector in a finite-dimensional space $Y$ that contains
all the experimental results, \eg\ the value of the structure function for
different values of the kinematics. In general we will assume that $\obs \in
\real^{\ndata}$, \ie\ we have a finite number $\ndata$ of real experimental
values. The quantity of interest is the composed operator
\begin{align}
  \fwdobsop : ~& \modelspace \to \real^{\ndata} \nonumber \\
                 & \fwdobsop = O \circ G\, ,
\end{align}
which maps the input $\modelvec$ to the set of data. Taking into account the fact that
experimental data are subject to noise, we can write
\begin{align}
  \label{eq:NoisyInverseProblem}
  \obs = \fwdobsop(\modelvec) + \obsnoise\, ,
\end{align}
where $\obsnoise$ is a random variable defined over $\real^{\ndata}$
with probability density $\rho(\obsnoise)$. We will refer to $\obsnoise$ as the
{\em observational noise}. The inverse problem becomes finding $\modelvec$
given $\obs$. It is often the case that inverse problems are ill-defined
in the sense that the solution may not exist, may not be unique, or
may be unstable under small variations of the problem. 

In solving this problem we are going to adopt a Bayesian point of view; our
prior knowledge about $\modelvec$ is encoded in a prior probability measure
$\mu_M^0$, and we use Bayes' theorem to compute the posterior probability
measure of $\modelvec$ given the data $\obs$, which we denote as
$\mu_M^\fwdobsop$. When possible, we denote the probability densities associated
to $\mu_M^0$ and $\mu_M^\fwdobsop$, by $\pi_M^0$ and $\pi_M^\fwdobsop$
respectively. Then, using Eq.~(\ref{eq:NoisyInverseProblem}), we can write the
data likelihood, \ie\ the probability density of $\obs$ given $\modelvec$,
\begin{align}
  \label{eq:YGivenUProbDensity}
  \pi(\obs|\modelvec) = \rho(\obs-\fwdobsop(\modelvec))\, ,
\end{align}
and Bayes' theorem yields
\begin{align}
  \label{eq:BayesThmInversePosterior}
  \pi_M^\fwdobsop(\modelvec) = \pi(\modelvec|\obs) \propto \rho(\obs-\fwdobsop(\modelvec)) \pi_M^0(\modelvec)\, .
\end{align}

\paragraph{Example}

Even though the concepts that we have introduced so far should sound familiar,
it is worthwhile to spend a few paragraphs to clarify some ideas and present an
explicit argument, where all the probability densities are carefully defined.
This is best exemplified by considering the case where both the observational
noise and the model prior are Gaussian. We assume that we are given a set of
central values $\obspriorcent \in \real^{\ndata}$ and their covariance matrix
$\obspriorcov$. Then the {\em prior} probability density of the observable
$\obs$ is 
\begin{equation}
  \label{eq:PriorData}
  \pi_{D}^0(\obs|\obspriorcent,\obspriorcov) \propto \exp\left(
    -\frac12 \left| \obs - \obspriorcent \right|_{\obspriorcov}^2
    \right)\, ,
\end{equation}
where the suffix $D$ emphasises the fact that this is a probability density in
data space, and the notation explicitly reminds us that this is the probability
density given the central values $\obspriorcent$ (and the covariance matrix).
Similarly we can choose a Gaussian distribution for the input model,
characterized by a central value $\modelpriorcent$ and a covariance
$\modelpriorcov$:
\begin{align}
  \label{eq:PiZeroGauss}
  \pi_{M}^0(\modelvec|\modelpriorcent,\modelpriorcov)  
  &\propto \exp\left(
              -\frac12 \left| \modelvec - \modelpriorcent \right|_{\modelpriorcov}^2
              \right)\, .
\end{align}
Following the convention above, we use a suffix $M$ here to remind the reader
that we are looking at a probability density in the space of models. Note that
in the expressions above we used the norms in $\modelspace$ and $\real^{\ndata}$
respectively, and introduced the short-hand notation
\begin{align}
  \left|a\right|_M^2 = \left| M^{-1/2} a\right|^2\, ,
\end{align}
where $a$ denotes a generic element of $\modelspace$, $R$ or $\real^{\ndata}$.
For the case where $a \in \real^{\ndata}$, we use the Euclidean norm and
\begin{align}
  \left| a \right|_M^2 = \sum_{i,j} a_i M_{ij} a_j\, ,
\end{align}
where the indices $i,j$ run from 1 to $\ndata$.  
Up to this point data and models are completely independent, and the joint
distribution is simply the product of $\pi_{D}^0$ and $\pi_{M}^0$. 

The forward map induces a correlation between the input model and the
observables, so we introduce a probability density $\theta$ that describes these
correlations due to the physical theory,  
\begin{equation}
  \label{eq:ThetaCorr}
  \theta(\obs,\modelvec|\fwdobsop) = \delta\left(\obs - \fwdobsop(\modelvec)\right)\, ,
\end{equation}
where the Dirac delta corresponds to the case where there are no theoretical
uncertainities. Theoretical uncertainties can be introduced by broadening the
distribution of $\obs$ away from the exact prediction of the forward map, \eg\
using a Gaussian with covariance $C_T$,
\begin{equation}
  \label{eq:TheoryErrors}
  \theta(\obs,\modelvec|\fwdobsop) = \exp\left(
    -\frac12 
    \left| \obs - \fwdobsop(\modelvec)
    \right|_{C_T}^2\right)\, .
\end{equation}
Note however that there are no rigorous arguments favouring the fact that
theoretical errors are normally distributed. Taking the correlation
$\theta(\obs,\modelvec|\fwdobsop)$ into account, the joint distribution of
$\obs$ and $\modelvec$ is
\begin{align}
  \label{eq:JointYAndU}
  \pi^\fwdobsop(\obs,\modelvec|\obspriorcent,\obspriorcov,\modelpriorcent,\modelpriorcov) 
  \propto 
  \pi_{D}^0(\obs|\obspriorcent,\obspriorcov) 
  \pi_{M}^0(\modelvec|\modelpriorcent, \modelpriorcov) 
  \theta(\obs,\modelvec|\fwdobsop)\, .
\end{align}
We can now marginalize with respect to \obs, neglecting theory errors, 
\begin{align}
  \label{eq:MarginOne}
  \pi^\fwdobsop_M(\modelvec|\obspriorcent,\obspriorcov,\modelpriorcent,\modelpriorcov) 
  &\propto \int dy\, \pi_{D}^0(\obs|\obspriorcent,\obspriorcov) 
    \pi_{M}^0(\modelvec|\modelpriorcent,\modelpriorcov) 
    \theta(\obs,\modelvec|\fwdobsop) \\
  & \propto \pi_{M}^0(\modelvec|\modelpriorcent,\modelpriorcov)  
    \int dy\, \pi_{D}^0(\obs|\obspriorcent,\obspriorcov) 
    \delta\left(\obs-\fwdobsop(\modelvec)\right) \\
  & \propto \pi_{M}^0(\modelvec|\modelpriorcent,\modelpriorcov) 
    \pi_{D}^0(\fwdobsop(\modelvec)|\obspriorcent,\obspriorcov)\, .
\end{align}
The log-likelihood in the Gaussian case is simply the $\chi^2$ of the data,
$\obspriorcent$, to the theory prediction, $\fwdobsop(\modelvec)$:
\begin{equation}
  \label{eq:LikelyChiSq}
  -\log\pi_D^0(\fwdobsop(\modelvec)|\obspriorcent) =  
      \frac12 \left|
      \fwdobsop(\modelvec) - \obspriorcent
      \right|_{\obspriorcov}^2
    \, .
\end{equation}
In the notation of Eq.~\ref{eq:BayesThmInversePosterior}
\begin{equation}
  \label{eq:IdentifyRho}
  \pi_D^0(\fwdobsop(\modelvec)|\obspriorcent) = \rho\left(
    \fwdobsop(\modelvec) - \obspriorcent
  \right)\, ,
\end{equation}
where in this case 
\begin{align}
  \label{eq:RhoGauss}
  \rho(\obsnoise) &\propto \exp\left(
               -\frac12 \left|\obsnoise\right|_{\obspriorcov}^2
               \right)\, .
\end{align}
The probability density
$\pi^\fwdobsop_M(\modelvec|\obspriorcent,\obspriorcov,\modelpriorcent,\modelpriorcov)$
was called $\pi_M^\fwdobsop(\modelvec)$ in
Eq.~\ref{eq:BayesThmInversePosterior}, where the suffix $\fwdobsop$ is a
short-hand to denote the posterior probability in model space, taking into
account all the conditional variables. Hence, for the Gaussian case, the result
from Bayes' theorem reduces to
\begin{align}
  \label{eq:PosteriorModel}
  \pi_M^\fwdobsop(\modelvec) 
  &\propto 
  \exp\left[
    -\frac12 \left| \obspriorcent - \fwdobsop(\modelvec) \right|_{\obspriorcov} ^2
    -\frac12 \left| \modelvec - \modelpriorcent \right|_{\modelpriorcov}^2
  \right] \\ 
  &= 
  \exp\left[
    - S(\modelvec)
  \right]\, .
\end{align}
Note that in the argument of the likelihood function we have the central values
of the data points $\obspriorcent$. Eq.~(\ref{eq:PosteriorModel}) is the
Bayesian answer to the inverse problem, our knowledge of the model $\modelvec$
is encoded in the probability measure $\mu_M^\fwdobsop$, which is fully
specified by the density $\pi_M^\fwdobsop$. There are several ways to
characterise a probability distribution. The NNPDF approach is focused on the
determination of the {\em Maximum A Posteriori (MAP)} estimator, \ie\ the
element $u_* \in \modelspace$ that maximises $\pi_M^\fwdobsop(\modelvec)$:
\begin{align}
  \label{eq:MAP}
  u_* = \arg\min_{\modelvec \in \modelspace} 
  \left(
  -\frac12 \left| \obspriorcent - \fwdobsop(\modelvec) \right|_{\obspriorcov}^2
  -\frac12 \left| \modelvec - \modelpriorcent \right|_{\modelpriorcov}^2
  \right)\, .
\end{align}
For every instance of the data $\obspriorcent$, the MAP estimator is computed by
minimising a regulated $\chi^2$, we will refer to this procedure as the {\em
classical fit} of experimental data to a model. Note that in the Bayesian
approach, the regulator appears naturally after having specified carefully all
the assumptions that enter in the prior. In this specific example the regulator
arises from the Gaussian prior for the model input $\modelvec$, which is
normally ditributed around a solution $\modelpriorcent$. The MAP estimator
provides the explicit connection between the Bayesian approach and the classical
fit.

\subsection{Comparison with classical fitting}
\label{sec:comp-class-fit}

There are a number of results that make the connection between the two
approaches more quantitative, and therefore more transparent. We are going to
summarise these results here without proofs, referring the reader to the
mathematical literature for the missing details. Working in the
finite-dimensional case, we assume 
\begin{align*}
  \modelvec &\in \real^{\nmodel} \, ,\\
  \obs &\in \real^{\ndata}\, ,
\end{align*}
and we are going to consider in detail two examples from Ref.~\cite{StuartCore},
which illustrate the role of the priors in the Bayesian setting. It is
particularly useful to distinguish the case of an underdetermined system from
the case of an overdetermined one. 

\paragraph{Underdetermined system}
The first case that we are going to analyse is the case of a linear system that
is underdetermined by the data. The linear model is completely specified by a
vector of coefficients $g\in \real^{\nmodel}$, 
\begin{equation}
  \label{eq:LinSyst}
  \mathcal{G}(u) = \left(g^T u\right)\, .
\end{equation}
Assuming that we have only one datapoint, \ie\ $\ndata=1$, 
\begin{equation}
  \label{eq:LinearModelEx}
  \obs = (g^T \modelvec) + \obsnoise\, ,
\end{equation}
where $\obsnoise \sim \mathcal{N}(0,\gamma^2)$ is one Gaussian number, whose
probability density is centred at $0$ and has variance $\gamma^2$. For
simplicity we are going to assume that the prior on $\modelvec$ is also a
multi-dimensional Gaussian, centred at $0$ with covariance matrix $\modelpriorcov$. In
this case the posterior distribution can be written as
\begin{equation}
  \label{eq:GaussPostExplicit}
    \pi_M^\fwdobsop(\modelvec) 
    \propto \exp \left[
      -\frac{1}{2\gamma^2} \left|\obs - (g^T \modelvec) \right|^2 - \frac12 \left|\modelvec\right|_{\modelpriorcov}^2 
    \right]\, ,
\end{equation}
which is still a Gaussian distribution for $\modelvec$. The mean and covariance
are respectively
\begin{align}
  m &= \frac{(\modelpriorcov g) \obs}{\gamma^2 + (g^T \modelpriorcov g)}\, , \\
  \Sigma &= \modelpriorcov - 
  \frac{(\modelpriorcov g) (\modelpriorcov g)^T}{\gamma^2 + (g^T \modelpriorcov g)}\, .
\end{align}
Because the argument of the exponential is a quadratic form, the mean of the
distribution coincides with the MAP estimator. It is instructive to look at
these quantities in the limit of infinitely precise data, \ie\ in the limit
$\gamma\to 0$:
\begin{align}
  m_\star &= 
  \lim_{\gamma\to 0} m
  = \frac{(\modelpriorcov g) \obs}{(g^T \modelpriorcov g)}\, , \\
  \Sigma_\star &= 
  \lim_{\gamma\to 0} \Sigma 
  = \modelpriorcov - 
  \frac{(\modelpriorcov g) (\modelpriorcov g)^T}{(g^T \modelpriorcov g)}\, .
\end{align}
These values satisfy
\begin{align}
  (g^T m_\star) = \obs \, , \\
  (\Sigma_\star g) = 0 \, ,
\end{align}
which shows that the mean of the distribution is such that the data point is
exactly reproduced by the model, and that the uncertainty in the direction
defined by $g$ vanishes. It should be noted that the uncertainty in directions
perpendicular to $g$ does not vanish and is determined by a combination of the
prior and the model, \viz\ $\modelpriorcov$ and $g$ in our example. This is a
particular example of a more general feature: for underdetermined systems the
information from the prior still shapes the probability distribution of the
solution even in the small noise limit.  

\paragraph{Overdetermined system}
We are now going to consider an example of an overdetermined system and discuss
again the case of small observational noise. We consider $\ndata\geq 2$ and
$n=1$, with a linear forward map such that
\begin{equation}
 \label{eq:OverDetForwMap}
 \obs = g \modelvec  + \obsnoise\, ,
\end{equation} 
where $\obsnoise$ is an $\ndata$-dimensional Gaussian variable with a diagonal
covariance $\gamma^2 I$, where $I$ denotes the identity matrix. For simplicity
we are going to assume a Gaussian prior with unit variance for $\modelvec$, which yields
for the posterior distribution:
\begin{equation}
  \label{eq:OverDetPost}
  \pi_M^\fwdobsop(\modelvec) 
    \propto 
    \exp\left(
      -\frac{1}{2\gamma^2} \left| \obs - g(\modelvec + \beta \modelvec^3)\right|^2
      -\frac12 \modelvec^2
    \right)\, .
\end{equation} 
If $\beta=0$ the posterior is Gaussian and we can easily compute its mean and variance: 
\begin{align}
  m &= \frac{(g^T \obs)}{\gamma^2 + |g|^2} \, , \\
  \sigma^2 &=
    \frac{\gamma^2}{\gamma^2 + |g|^2}\, .
\end{align}
In this case, in the limit of vanishing observational noise, we obtain
\begin{align}
  m_\star &= \frac{(g^T \obs)}{|g|^2} \, ,\\
  \sigma_\star^2 &= 0\, .
\end{align}
The mean is given by the weighted average of the datapoints, which is also the solution of the $\chi^2$ minimization
\begin{equation}
  m_\star = \arg\min_{\modelvec\in\real} \left|\obs - g \modelvec\right|^2\, .
\end{equation}
Note that in this case the variance $\sigma_\star$ vanishes independently of the
prior. In the limit of small noise, the distribution tends to a Dirac delta
around the value of the MAP estimator.  

\subsection{Linear Problems}
\label{sec:LinProbs}

Linear problems in finite-dimensional spaces are characterized by a simple forward law, 
\begin{equation}
  \label{eq:MatrixG}
  \obs = \linmap \modelvec\, ,
\end{equation}
where $\linmap$ is a matrix. In this framework one can readily  derive
analytical solutions that are useful to understand the main features of the
Bayesian approach. Assuming that the priors are Gaussian again, the cost
function $S(\modelvec)$ is a quadratic function of $\modelvec$,
\begin{align}
  \label{eq:CostLinGauss}
  S(\modelvec) &= 
  \left(\linmap \modelvec - \obspriorcent \right)^T \obspriorcov^{-1} 
  \left(\linmap \modelvec - \obspriorcent \right) + 
  \left( \modelvec - \modelpriorcent \right)^T \modelpriorcov^{-1} \left(\modelvec - \modelpriorcent \right) \\
  &= 
  \left(\modelvec - \modelpostcent\right) \modelpostcov^{-1}
  \left(\modelvec - \modelpostcent\right) + \mathrm{const}\, ,
\end{align} 
where
\begin{align}
  \label{eq:PostParamsCov}
  \modelpostcov^{-1} &= 
  \left(
    \linmap^T \obspriorcov^{-1} \linmap + \modelpriorcov^{-1}
  \right)\, , \\
  \label{eq:PostParamsMean}
  \modelpostcent &=
  \modelpostcov  \left(
    \linmap^T \obspriorcov^{-1} \obspriorcent + \modelpriorcov^{-1} \modelpriorcent
  \right)\, .
\end{align}
The case where we have no prior information on the model is recovered by taking
the limit $\modelpriorcov^{-1} \to 0$, which yields
\begin{align}
  \label{eq:NoPriorLinModel}
  \modelpostcov^{-1} &= 
  \left(
    \linmap^T \obspriorcov^{-1} \linmap
  \right)\, , \\
  \modelpostcent &=
  \modelpostcov  \left(
    \linmap^T \obspriorcov^{-1} \obspriorcent 
  \right)\, . \label{eq:NoPriorLinModelCov}
\end{align}
The action of $\obspriorcov^{-1}$ on the vector of observed data $\obspriorcent$
is best visualised using a spectral decomposition
\begin{equation}
  \label{eq:CDSpecDec}
  \obspriorcov^{-1} = \sum_n \frac{1}{\sigma_n^2} P_n\, ,
\end{equation}
where $P_n$ denotes the projector on the $n$-th eigenvector of $\obspriorcov$, and
$\sigma_n^2$ is the corresponding eigenvalue. The action of $\obspriorcov^{-1}$ is to
perform a weighted average of the components of $\obspriorcent$ in the directions of the
eigenvectors of $\obspriorcov$.

An explicit expression for the posterior distribution of data can be obtained
from the joint distribution by marginalising over the model input $\modelvec$:
\begin{align}
  \label{eq:PosteriorDataSpace}
  \pi_D^\fwdobsop(\obs|\obspriorcent,\obspriorcov,\modelpriorcent,\modelpriorcov)
  &= \int du\, 
  \pi^\fwdobsop(\obs,\modelvec|\obspriorcent,\obspriorcov,\modelpriorcent,\modelpriorcov) \\
  &\propto \exp\left(
    -\frac12 \left(\obs - \obspostcent\right)^T \obspostcov^{-1}
    \left(\obs - \obspostcent\right)
  \right)\, ,
\end{align}
where
\begin{align}
  \label{eq:PosteriorDataParamsMean}
  \obspostcent &= \linmap \modelpostcent\, , \\
  \label{eq:PosteriorDataParamsCov}
  \obspostcov &= \linmap \modelpostcov \linmap^T\, .
\end{align}

\paragraph{Posterior distribution of unseen data}

In real-life cases we are also interested in the posterior distribution of a set
of data that have not been included in the fit. In the Bayesian framework that
we have developed this can be modeled by having two sets of data $y$ and $y'$,
for which we have a prior distribution 
\begin{align}
  \label{eq:JointIndepDataPrior}
  \pi_D^0&\left(y,y'|y_0,C_D,y'_0,C'_D\right) 
   = \pi_D^0\left(y'|y'_0,C'_D\right) \pi_D^0\left(y|y_0,C_D\right) \\
  & \propto 
  \exp\left[-\frac12 \left(y'-y'_0\right)^T (C'_D)^{-1} 
  \left(y'-y'_0\right)\right]\, 
  \exp\left[-\frac12 \left(y-y_0\right)^T (C_D)^{-1} 
  \left(y-y_0\right)\right]\, .
\end{align}
Following the derivation above, we can write the joint distribution for the data
and the model 
\begin{equation}
  \label{eq:JointModelData}
  \pi^\fwdobsop(y,y',u) 
  \propto 
  \pi_D^0(y,y'|y_0,C_D,y'_0,C'_D) \pi_M^0(u) 
  \delta\left(y - \mathcal{G}u\right)
  \delta\left(y'- \mathcal{G}'u\right)\, .
\end{equation}
Note that because both sets of data are derived from the same model $u$, the
joint distribution above introduces a correlation between the data sets. 

We can now marginalise with respect to the dataset $y$, 
\begin{equation}
  \label{eq:MarginaliseDatasetY}
  \begin{split}
    \pi(y',u) 
    \propto &
    \exp\left[-\frac12 \left(y'-y'_0\right)^T (C'_D)^{-1} 
    \left(y'-y'_0\right)\right]\, 
    \exp\left[-\frac12 \left(u-\tilde{u}\right)^T (\tilde{C}_M)^{-1} 
    \left(u-\tilde{u}\right)\right] \\
    & \quad \times \delta\left(y'- \mathcal{G}'u\right)\, .
  \end{split}
\end{equation}
where $\tilde{C}_M$ and $\tilde{u}$ are given respectively in
Eqs.~\ref{eq:PostParamsCov} and \ref{eq:PostParamsMean}. By marginalising again,
this time with respect to the model, we derive the posterior distribution of the
unseen data,
\begin{equation}
  \label{eq:MarginaliseModelU}
  \pi^y_D(y') \propto 
  \exp \left[ 
   \frac12 \left(y' - \tilde{y}'\right)^T
   (\tilde{C}'_D)^{-1} 
   \left(y' - \tilde{y}'\right)
   \right]\, ,
\end{equation}
where
\begin{align}
  \tilde{C}'_D 
  &= \mathcal{G}' \tilde{C}'_M \mathcal{G}'^{T} \\
  \tilde{y}'
  &= \mathcal{G}' \tilde{u}'\, ,
\end{align}
and 
\begin{align}
  \tilde{C}_M'^{-1} 
  &= \mathcal{G}'^T C_D'^{-1} \mathcal{G}' + \tilde{C}_M^{-1} \\
  \tilde{u}' 
  &= \tilde{C}'_M \left(
    \mathcal{G}'^T C_D'^{-1} y_0' + \tilde{C}_M^{-1} \tilde{u} 
    \right) \\
  \tilde{C}_M^{-1}
  &= \mathcal{G}^T C_D^{-1} \mathcal{G} + C_M^{-1} \\
  \tilde{u}
  &= \tilde{C}_M \left(
    \mathcal{G}^T C_D^{-1} y_0 + C_M^{-1} u_0
  \right)\, .
\end{align}

\paragraph{Non-linear Models}

The linear models that we have discussed so far may look over-simplified at
first sight. In practice, it turns out that non-linear models can often be
linearised around the central value of the prior distribution, 
\begin{equation}
  \label{eq:LinU0}
  \fwdobsop(\modelvec) = \fwdobsop(\modelpriorcent) + G \left(\modelvec - \modelpriorcent\right) + \ldots\, ,
\end{equation}
where 
\begin{equation}
  \label{eq:FirstDerU0}
  G^i_\alpha = \left. \frac{\partial \fwdobsop^i}{\partial u_\alpha} \right|_{\modelpriorcent}\, ,
\end{equation}
and we have neglected higher-order terms in the expansion of
$\fwdobsop(\modelvec)$.

If these terms are not negligible, another option is to find the MAP estimator,
and then expand the the forward map around it, which yields equations very
similar to the previous ones, with $\modelpriorcent$ replaced by $u_*$. If the
posterior distribution of $u$ is sufficiently peaked around the
MAP estimator, then the linear approximation can be sufficiently accurate. 


\subsection{The infinite-dimensional case}
\label{sec:infin-dimens-case}

In the finite-dimensional case, where the probability measures are specified by
their densities with respect to the Lebesgue measure,
Eq.~(\ref{eq:BayesThmInversePosterior}) can be rephrased by saying  that $\rho$
is the Radon-Nikodym derivative of the probability measure $\mu^\fwdobsop$ with
respect to $\mu_0$, \viz
\begin{align}
  \label{eq:RadonNikodym}
  \frac{d\mu^\fwdobsop}{d\mu^0} (\modelvec) \propto \rho(\obs-\fwdobsop(\modelvec)) \pi^0(\modelvec)\, .
\end{align}
Finally, using the fact that the density $\rho$ is a positive
function, we can rewrite 
\begin{align}
  \label{eq:PotentialDef}
  \rho(\obs-\fwdobsop(\modelvec)) = \exp\left(-\Phi(\modelvec;\obs)\right)\, ,
\end{align}
and therefore
\begin{align}
  \label{eq:RadonNikodymTwo}
  \frac{d\mu^\fwdobsop}{d\mu^0} (\modelvec) \propto \exp\left(-\Phi(\modelvec;\obs)\right)\, .
\end{align}
In finite-dimensional spaces, the three equations above are just definitions
that do not add much content. Their interest resides in the fact that the last
expression, Eq.~(\ref{eq:RadonNikodymTwo}), can be properly defined when $\modelspace$ is
infinite-dimensional, allowing a rigorous extension of the Bayesian formulation
of inverse problems to the case of infinite-dimensional spaces. 

Adopting a heuristic approach, we can say that a function $f$ is a random
function is $f(x)$ is a random variable for all values of $x$. Since the values
of the function at different values of $x$ can be correlated, a random function
is fully characterised by specifying the joint probability densities
\begin{equation}
  \label{eq:RandomFuncJointProb}
  \pi\left(
    f_1, \ldots, f_n; x_1, \ldots x_n
  \right)\, ,
\end{equation}
where $f_i=f(x_i)$, for all values of $n$, and all values of $x_1, \ldots, x_n$.
These finite-dimensional densities allow the definition of a probability
measure, under some regularity hypotheses that we will not investigate here.
Similarly to what happens in the finite-dimensional case, a Gaussian random
function is completely characterised by the two-dimensional probability
densities $\pi(x_1,x_2;t_1,t_2)$.

\paragraph{Functional linear problems} This formalism allows us to formulate a Bayesian solution of the inverse problem 
\begin{equation}
  \label{eq:BayesLinearInverse}
  y^i = \int dx\, G^i(x) u(x)\, ,
\end{equation}
where $y_i$ is a discrete set of oobservables and $u(x)$ is Gaussian random
function, with prior mean $u_0(x)$ and covariance $C_M(x,x')$. The vector of
observed values is denoted $y_0$, and we assume that the prior distribution of
$y$ is a Gaussian centred at $y_0$ with covariance $C_D$. We introduce the
matrix
\begin{equation}
  \label{eq:Smatrix}
  S^{ij} =
  \int dx dx'\, G^i(x) C_M(x,x') G^j(x') + C_D^{ij}\, ,
\end{equation}
and its inverse $T_{ij}=\left(S^{-1}\right)^{ij}$. Just like in the finite-dimensional example discussed above, the posterior Gaussian field is centred at
\begin{align}
  \label{eq:PostMeanFunc}
  \tilde{u}(x) = u_0(x) + 
  \int dx'\, C_M(x,x') G^i(x') T_{ij} \left(
    y_0^j - \int dx''\, G^j(x'') u_0(x'') 
  \right)\, .
\end{align}
Interestingly, this can be rewritten as
\begin{equation}
  \label{eq:TowardsBackus}
  \tilde{u}(x) = u_0(x) + 
  \int dx'\, C_M(x,x') \Psi(x')\, ,
\end{equation}
where 
\begin{eqnarray}
  \label{eq:PsiDef}
  \Psi(x) = G^i(x) \delta y^i\, ,
\end{eqnarray}
and the weighted residuals are given by
\begin{equation}
  \label{eq:DeltaYDef}
  \delta y^i = T_{ij} \left(
  y_0^j - \int dx'\, G^j(x') u_0(x')
  \right)\, .
\end{equation}
Using the notation introduced above, the posterior covariance can be written as
\begin{equation}
  \label{eq:PostCovFunc}
  \tilde{C}_M(x,x') = 
  C_M(x,x') - \Psi^i(x) T_{ij} \Psi^j(x')\, .
\end{equation}

The Bayesian result summarised above can be compared with the method proposed by Backus and Gilbert to solve the same problem. Assuming that there exists an unknown 'true' model $\utrue$, such that the observed data are
\begin{equation}
  \label{eq:BackStart}
  y_0^i = \int dx\, G^i(x) \utrue\, ,
\end{equation}
we look for an estimate $\uest$ of the true solution in the form
\begin{equation}
  \label{eq:BackAnsatz}
  \uest(x) = Q^i(x) y^i_0\, .
\end{equation}
Using Eq.~\ref{eq:BackStart} we obtain
\begin{equation}
  \label{eq:BackFilter}
  \uest(x) = \int dx' R(x,x') \utrue(x')\, , 
\end{equation}
which states that with a finite amount of data we can only resolve a filtered
version of the true solution. The kernel $R$ is given by
\begin{equation}
  \label{eq:BackKernel}
  R(x,x') = Q^i(x) G^i(x')\, .
\end{equation}
The coefficients $Q(x)$ can be chosen so that the kernel is as close as possible
to a delta function:
\begin{equation}
  \label{eq:BackDelta}
  R(x,x') \simeq \delta(x,x') ~~ \Longrightarrow ~~
  \uest \simeq \utrue\, .
\end{equation}
Approximating the delta function can be achived by minimising 
\begin{equation}
  \label{eq:BackDeltaness}
  \int dx'\, \left(
    R(x,x') - \delta(x-x')
  \right)^2\, ,
\end{equation}
which yields
\begin{equation}
  \label{eq:BackSolution}
  Q^i(x) = \left(S^{-1}\right)^{ij} G^j(x)\, ,
\end{equation}
where 
\begin{equation}
  \label{eq:BackSMatrix}
  S^{ij} = \int dx\, G^i(x) G^j(x)\, .
\end{equation}
The intersting observation is that the central value of the Bayesian solution
presented above reduces to the Backus-Gilbert $\uest$ in the case where there is
no prior for the model, \ie\ if we assume that $u_0$ is just white noise and
therefore
\begin{equation}
  \label{eq:BackComparison}
  C_M(x,x') = \delta(x-x')\, .
\end{equation}
The Bayesian solution allows a variety of prior to be explicitly declared and
tested compared to the Backus-Gilbert solution. 

\section{Inverse Problems}
\label{sec:inverse-problems}

The problem of determining PDFs from a set of experimental data falls
under the general category of {\em inverse problems}, \ie\ the problem
of finding the input to a given model knowing a (finite) set of
observations. The model is specified by a {\em forward map}
\begin{align}
  \label{eq:ForwardMap}
  G : ~& X \to R \nonumber \\
      & u \mapsto r=G(u) \, ,
\end{align}
which associates a response $r \in R$ to the input $u \in X$, where we
assume that $X$ and $R$ are Banach spaces.~\footnote{Banach spaces are
  complete normed vector spaces. We do not need to get into a more
  detailed discussion here.} As an example we can think of $u$ as
being a PDF and $r$ a DIS structure function:
\begin{align}
  \label{eq:DISExample}
  r(x,Q^2) = \int dz\, C(x/z) u(z)\, .
\end{align}
Note that in this example the forward map maps one real function into
another real function. Experiments will not have access to the full
function $r$ but only to a subset of $\ndata$ observations. In order
to have a formal mathematical expression to take into account the fact
that we have a finite number of meaasurements, we introduce an {\em
  observation operator}
\begin{align}
  O : ~& R \to \mathbb{R}^{\ndata} \nonumber \\
       & r \mapsto y \, ,
\end{align}
where $y \in \mathbb{R}^{\ndata}$ is a vector that contains all the
experimental results, \eg\ the value of the structure function for
different values of the kinematics. The quantity of interest is the
composed operator
\begin{align}
  \mathcal{G} : ~& X \to \mathbb{R}^{\ndata} \nonumber \\
                 & \mathcal{G} = O \circ G\, ,
\end{align}
which maps the input to the forward map to the set of data. Taking
into account the fact that experimental data are subject to noise, we
can write
\begin{align}
  \label{eq:NoisyInverseProblem}
  y = \mathcal{G}(u) + \eta\, ,
\end{align}
where $\eta$ is a random variable defined over $\mathbb{R}^{\ndata}$
with probability density $\rho(\eta)$. We will refer to $\eta$ as the
{\em observational noise}. The inverse problem becomes finding $u$
given $y$. It is often the case that inverse problems are ill-defined
in the sense that the solution may not exist, may not be unique, or
may be unstable under small variations of the problem. 

In this context we are going to adopt a Bayesian point of view; our
prior knowledge about $u$ is encoded in a prior probability measure
$\mu_0$, and we use Bayes' theorem to compute the posterior
probability measure of $u$ given the data $y$, which we denote as
$\mu^y$. We denote the probability densities associated to $\mu_0$ and
$\mu^y$ by $\pi_0$ and $\pi^y$ respectively. Then, using
Eq.~(\ref{eq:NoisyInverseProblem}), we can write the data likelihood,
\ie\ the probability density of $y$ given $u$,
\begin{align}
  \label{eq:YGivenUProbDensity}
  \rho(y|u) = \rho(y-\mathcal G(u))\, ,
\end{align}
and Bayes' theorem yields
\begin{align}
  \label{eq:BayesThmInversePosterior}
  \pi^y(u) \propto \rho(y-\mathcal G(u)) \pi_0(u)\, .
\end{align}

The concepts that we have introduced are very familiar are best exemplified
by considering the case where both the observational noise and the
prior are Gaussian:
\begin{align}
  \label{eq:RhoGauss}
  \rho(\eta) &\propto \exp\left(
               -\frac12 \left|\eta\right|_C^2
               \right)\, , \\
  \label{eq:PiZeroGauss}
  \pi_0(u)  &\propto \exp\left(
              -\frac12 \left|(u-m_0)\right|_{\Sigma_0}^2
              \right)\, .
\end{align}
Note that in the above expressions we used the norms in $X$ and
$\mathbb{R}^{\ndata}$ respectively, and introduced the short-hand
notation
\begin{align}
  \left|v\right|_M^2 = \left| M^{-1/2} v\right|^2\, .
\end{align}
For the case where $v \in \mathbb{R}^{\ndata}$, we can use the usual
Euclidean norm  and
\begin{align}
  \left|v\right|_M^2 = \sum_{i,j} v_i M_{ij} v_j\, ,
\end{align}
and the indices $i,j$ run from 1 to $\ndata$.  The matrix $C$ is the
covariance of the experimental data, while $\Sigma_0$ is the
covariance of the Gaussian prior for $u$. Using Bayes' theorem we
obtain
\begin{align}
  \label{eq:PosteriorModel}
  \pi^y(u) \propto 
  \exp\left(
  -\frac12 \left|y - \mathcal G(u)\right|_C^2
  -\frac12 \left|(u-m_0)\right|_{\Sigma_0}^2
  \right)\, .
\end{align}
Eq.~(\ref{eq:PosteriorModel}) is the Bayesian answer to the inverse
problem, our knowledge of the model $u$ is encoded in the probability
measure $\mu^y$, which is fully specified by the density
$\pi^y$. There are several ways to characterise a probability
distribution. Here we will focus on the {\em Maximum A Posteriori
  (MAP)} estimator, \ie\ the element $u \in X$ that maximises
$\pi^y(u)$:
\begin{align}\label{eq:MAP}
  u^* = \arg\min_{u \in X} 
  \left(
  -\frac12 \left|y - \mathcal G(u)\right|_C^2
  -\frac12 \left|(u-m_0)\right|_{\Sigma_0}^2
  \right)\, .
\end{align}
For every instance of the data $y$, the MAP estimator is computed by
minimising a regulated $\chi^2$, we will refer to this procedure as
the {\em classical fit} of experimental data to a model. Note that in
the Bayesian approach, the regulator is provided by specifying
carefully all the assumptions that enter in the prior. The MAP
estimator provides the explicit connection between the Bayesian
approach and the classical fit.

\subsection{Comparison with classical fitting}
\label{sec:comp-class-fit}

There are a number of results that make the connection between the two
approaches more quantitative, and therefore more transparent. We are
going to summarise these results here without proofs, referring the
reader to the mathematical literature for the missing details. Working
in the finite-dimensional case, we assume 
\begin{align*}
  u &\in \mathbb{R}^n \, ,\\
  y &\in \mathbb{R}^{\ndata}\, ,
\end{align*}
and we are going to consider in detail two examples from Ref.~\cite{StuartCore}.

\paragraph{Underdetermined system}
The first case that we are going to analyse is the case of a linear system that
is underdetermined by the data. The linear model is completely specified by a
vector of coefficients $g\in \mathbb{R}^n$, while we are going to assume that we
only have one datapoint, \ie\ $\ndata=1$, and hence:
\begin{equation}
  \label{eq:LinearModelEx}
  y = (g^T u) + \eta\, ,
\end{equation}
where $\eta \sim \mathcal{N}(0,\gamma^2)$ is one Gaussian number, whose
probability density is centred at $0$ and has variance $\gamma^2$. For
simplicity we are going to assume that the prior on $u$ is also a
multi-dimensional Gaussian, centred at $0$ with covariance matrix $\Sigma_0$. In
this case the posterior distribution can be written as
\begin{equation}
  \label{eq:GaussPostExplicit}
    \pi^y(u) \propto \exp \left[
    \frac{1}{2\gamma^2} \left|y - (g^T u) \right|^2 - \frac12 \left|
      u
    \right|_{\Sigma_0}^2 
    \right]\, ,
\end{equation}
which is still a Gaussian distribution for $u$. The mean and covariance are
respectively
\begin{align}
  m &= \frac{(\Sigma_0 g) y}{\gamma^2 + (g^T \Sigma_0 g)}\, , \\
  \Sigma &= \Sigma_0 - 
  \frac{(\Sigma_0 g) (\Sigma_0 g)^T}{\gamma^2 + (g^T \Sigma_0 g)}\, .
\end{align}
It is instructive to look at these quantities in the limit of infinitely precise data, \ie\ in the limit $\gamma\to 0$:
\begin{align}
  m_\star &= 
  \lim_{\gamma\to 0} m
  = \frac{(\Sigma_0 g) y}{(g^T \Sigma_0 g)}\, , \\
  \Sigma_\star &= 
  \lim_{\gamma\to 0} \Sigma 
  = \Sigma_0 - 
  \frac{(\Sigma_0 g) (\Sigma_0 g)^T}{(g^T \Sigma_0 g)}\, .
\end{align}
These values satisfy
\begin{align}
  (g^T m_\star) = y \, , \\
  (\Sigma_\star g) = 0 \, ,
\end{align}
which shows that the mean of the distribution is such that the data point is
exactly reproduced by the model, and that the uncertainty in the direction
defined by $g$ vanishes. It should be noted that the uncertainty in directions
perpendicular to $g$ does not vanish and is determined by a combination of the
prior and the model, \viz\ $\Sigma_0$ and $g$ in our example. This is a
particular example of a more general feature: for underdetermined systems the
information from the prior still shapes the probability distribution of the
solution even in the small noise limit.  

\paragraph{Overdetermined system}
We are now going to consider an example of an overdetermined system and discuss
again the case of small observational noise. We consider $\ndata\geq 2$ and
$n=1$, with a forward map such that
\begin{equation}
 \label{eq:OverDetForwMap}
 y = g(u + \beta u^3) + \eta\, ,
\end{equation} 
where $\eta$ is an $\ndata$-dimensional Gaussian variable with a diagonal
covariance $\gamma^2 I$, where $I$ denotes the identity matrix. For simplicity
we are going to assume a Gaussian prior with unit variance for $u$, which yields
for the posterior distribution:
\begin{equation}
  \label{eq:OverDetPost}
  \pi^y(u) \propto 
    \exp\left(
      -\frac{1}{2\gamma^2} \left| y - g(u + \beta u^3)\right|^2
      -\frac12 u^2
    \right)\, .
\end{equation} 
If $\beta=0$ the posterior is Gaussian and we can easily compute its mean and variance: 
\begin{align}
  m &= \frac{(g^T y)}{\gamma^2 + |g|^2} \, , \\
  \sigma^2 &=
    \frac{\gamma^2}{\gamma^2 + |g|^2}\, .
\end{align}
In this case, in the limit of vanishing observational noise, we obtain
\begin{align}
  m_\star &= \frac{(g^T y)}{|g|^2} \, ,\\
  \sigma_\star^2 &= 0\, .
\end{align}
The mean is given by the weighted average of the datapoints, which is also the solution of the $\chi^2$ minimization
\begin{equation}
  m_\star = \arg\min_{u\in\mathbb{R}} \left|y - g u\right|^2\, .
\end{equation}
Note that in this case the variance $\sigma_\star$ vanishes independently of the prior. In the limit of small noise, the distribution tends to a Dirac delta around the value of the MAP estimator.  


\subsection{The infinite-dimensional case}
\label{sec:infin-dimens-case}

In the finite-dimensional case, where the probability measures are
specified by their densities with respect to the Lebesgue measure,
Eq.~(\ref{eq:BayesThmInversePosterior}) encodes the fact that $\rho$
is the Radon-Nikodym derivative of the probability measure $\mu^y$
with respect to $\mu_0$, \viz
\begin{align}
  \label{eq:RadonNikodym}
  \frac{d\mu^y}{d\mu_0} (u) \propto \rho(y-\mathcal G(u)) \pi_0(u)\, .
\end{align}
Finally, using the fact that the density $\rho$ is a positive
function, we can rewrite 
\begin{align}
  \label{eq:PotentialDef}
  \rho(y-\mathcal G(u)) = \exp\left(-\Phi(u;y)\right)\, ,
\end{align}
and therefore
\begin{align}
  \label{eq:RadonNikodymTwo}
  \frac{d\mu^y}{d\mu_0} (u) \propto \exp\left(-\Phi(u;y)\right)\, .
\end{align}
In finite-dimensional spaces, the three equations above are just
definitions that do not add much content. Their interest resides in
the fact that the last expression, Eq.~(\ref{eq:RadonNikodymTwo}), can
be properly defined when $X$ is infinite-dimensional.


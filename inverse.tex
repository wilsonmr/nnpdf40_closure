
\section{Inverse Problems}
\label{sec:inverse-problems}

The problem of determining PDFs from a set of experimental data falls under the
general category of {\em inverse problems}, \ie\ the problem of finding the
input to a given model knowing a set of observations, which are often finite and
noisy. The space of inputs is denoted by $X$, while $R$ denotes the space of responses. The model is specified by a {\em
forward map}
\begin{align}
  \label{eq:ForwardMap}
  G : ~& X \to R \nonumber \\
      & u \mapsto r=G(u) \, ,
\end{align}
which associates a response $r \in R$ to the input $u \in X$, where we assume
that $X$ and $R$ are Banach spaces.~\footnote{Banach spaces are complete normed
vector spaces. We do not need to get into a more detailed discussion here.} As
an example we can think of $u$ as being a Parton Distribution Function, \ie\ a
function defined on the interval $[0,1]$, and $r$ a DIS structure function:
\begin{align}
  \label{eq:DISExample}
  r(x,Q^2) = \int dz\, C(x/z) u(z)\, .
\end{align}
Note that in this example the forward map maps one real function into
another real function. Experiments will not have access to the full
function $r$ but only to a subset of $\ndata$ observations. In order
to have a formal mathematical expression to take into account the fact
that we have a finite number of meaasurements, we introduce an {\em
  observation operator}
\begin{align}
  O : ~& R \to Y \nonumber \\
       & r \mapsto y \, ,
\end{align}
where $y \in Y$ is a vector in a finite-dimensional space $Y$ that contains all
the experimental results, \eg\ the value of the structure function for different
values of the kinematics. In general we will assume that $y \in
\mathbb{R}^{\ndata}$, \ie\ we have a finite number $\ndata$ of real experimental
values. The quantity of interest is the composed operator
\begin{align}
  \mathcal{G} : ~& X \to \mathbb{R}^{\ndata} \nonumber \\
                 & \mathcal{G} = O \circ G\, ,
\end{align}
which maps the input to the forward map to the set of data. Taking
into account the fact that experimental data are subject to noise, we
can write
\begin{align}
  \label{eq:NoisyInverseProblem}
  y = \mathcal{G}(u) + \eta\, ,
\end{align}
where $\eta$ is a random variable defined over $\mathbb{R}^{\ndata}$
with probability density $\rho(\eta)$. We will refer to $\eta$ as the
{\em observational noise}. The inverse problem becomes finding $u$
given $y$. It is often the case that inverse problems are ill-defined
in the sense that the solution may not exist, may not be unique, or
may be unstable under small variations of the problem. 

In this context we are going to adopt a Bayesian point of view; our
prior knowledge about $u$ is encoded in a prior probability measure
$\mu^0$, and we use Bayes' theorem to compute the posterior
probability measure of $u$ given the data $y$, which we denote as
$\mu^y$. We denote the probability densities associated to $\mu^0$ and
$\mu^y$ by $\pi^0$ and $\pi^y$ respectively. Then, using
Eq.~(\ref{eq:NoisyInverseProblem}), we can write the data likelihood,
\ie\ the probability density of $y$ given $u$,
\begin{align}
  \label{eq:YGivenUProbDensity}
  p(y|u) = \rho(y-\mathcal G(u))\, ,
\end{align}
and Bayes' theorem yields
\begin{align}
  \label{eq:BayesThmInversePosterior}
  \pi^y(u) \propto \rho(y-\mathcal G(u)) \pi^0(u)\, .
\end{align}

\paragraph{Example}

The concepts that we have introduced so far should sound familiar, and are best
exemplified by considering the case where both the observational noise and the
model prior are Gaussian. We assume that we are given a set of central values
$y_0 \in \mathbb{R}^{\ndata}$ and their covariance matrix $C$. Then the {\em
prior} probability density of the observable $y$ is 
\begin{equation}
  \label{eq:PriorData}
  \pi_{D}^0(y|y_0) \propto \exp\left(
    -\frac12 \left| y - y_0 \right|_C^2
    \right)\, ,
\end{equation}
where the suffix $D$ emphasises the fact that this is a probability density in
data space, and the notation explicitly reminds us that this is the probability
density given the central values $y_0$ (and the covariance matrix). Similarly we
can choose a Gaussian distribution for the input model, characterized by a
central value $u_0$ and a covariance $\Sigma_0$:
\begin{align}
  \label{eq:PiZeroGauss}
  \pi_{M}^0(u|u_0)  &\propto \exp\left(
              -\frac12 \left| u - u_0 \right|_{\Sigma_0}^2
              \right)\, .
\end{align}
Following the convention above, we use a suffix $M$ here to remind the reader
that we are looking at a probability density in the space of models. Note that
in the expressions above we used the norms in $X$ and $\mathbb{R}^{\ndata}$
respectively, and introduced the short-hand notation
\begin{align}
  \left|a\right|_M^2 = \left| M^{-1/2} a\right|^2\, ,
\end{align}
where $a$ denotes a generic element of $X$, $R$ or $\mathbb{R}^{\ndata}$. For
the case where $a \in \mathbb{R}^{\ndata}$, we can use the usual Euclidean norm
and
\begin{align}
  \left| a \right|_M^2 = \sum_{i,j} a_i M_{ij} a_j\, ,
\end{align}
where the indices $i,j$ run from 1 to $\ndata$.  
Up to this point data and models are completely independent, and the joint
distribution is simply the product of $\pi_{D}^0$ and $\pi_{M}^0$. 

The correlation between the input model and the observables is induced by the
forward map, 
\begin{equation}
  \label{eq:ThetaCorr}
  \theta(y,u|\mathcal{G}) = \delta\left(y - \mathcal{G}(u)\right)\, ,
\end{equation}
where the Dirac delta corresponds to the case where there are no theoretical
uncertainities. Theoretical uncertainties can be introduced by broadening the
distribution of $y$ away from the exact prediction of the forward map, \eg\
using a Gaussian with covariance $C_T$,
\begin{equation}
  \label{eq:TheoryErrors}
  \theta(y,u|\mathcal{G}) = \exp\left(
    -\frac12 
    \left| y - \mathcal{G}(u)
    \right|_{C_T}^2\right)\, .
\end{equation}
Taking the correlation into account, the joint distribution of $y$ and $u$ is
\begin{align}
  \label{eq:JointYAndU}
  \pi(y,u|y_0,u_0,\mathcal{G}) \propto 
  \pi_{D}^0(y|y_0) \pi_{M}^0(u|u_0) \theta(y,u|\mathcal{G})\, .
\end{align}
We can now marginalize with respect to y, neglecting theory errors, 
\begin{align}
  \label{eq:MarginOne}
  \pi_M(u|y_0,u_0,\mathcal{G}) 
  &\propto \int dy\, \pi_{D}^0(y|y_0) \pi_{M}^0(u|u_0) 
    \theta(y,u|\mathcal{G}) \\
  & \propto \pi_{M}^0(u|u_0)  \int dy\, \pi_{D}^0(y|y_0) 
    \theta(y,u|\mathcal{G}) \\
  & \propto \pi_{M}^0(u|u_0) \pi_{D}^0(\mathcal{G}(u)|y_0)\, .
\end{align}
The log-likelihood in the Gaussian case is simply the $\chi^2$ of the data to
the theory prediction:
\begin{equation}
  \label{eq:LikelyChiSq}
  -\log\pi_D^0(\mathcal{G}(u)|y_0) =  
      \frac12 \left|
      \mathcal{G}(u) - y_0
      \right|_C^2
    \, .
\end{equation}
In the notation of Eq.~\ref{eq:BayesThmInversePosterior}
\begin{equation}
  \label{eq:IdentifyRho}
  \pi_D^0(\mathcal{G}(u)|y_0) = \rho\left(
    \mathcal{G}(u) - y_0
  \right)\, ,
\end{equation}
where in this case 
\begin{align}
  \label{eq:RhoGauss}
  \rho(\eta) &\propto \exp\left(
               -\frac12 \left|\eta\right|_C^2
               \right)\, .
\end{align}
The probability density $\pi_M(u|y_0,u_0,\mathcal{G})$ was called $\pi^y(u)$ in
Eq.~\ref{eq:BayesThmInversePosterior}, where the suffix $y$ is a short-hand to
denote the posterior probability in model space, taking into account all the
conditional variables. Hence, for the Gaussian case, Bayes' theorem yields
\begin{align}
  \label{eq:PosteriorModel}
  \pi^y(u) \propto 
  \exp\left(
  -\frac12 \left| y_0 - \mathcal G(u) \right|_C^2
  -\frac12 \left| u - u_0 \right|_{\Sigma_0}^2
  \right)\, .
\end{align}
Note that in the argument of the likelihood function we have the central values
of the data points $y_0$. Eq.~(\ref{eq:PosteriorModel}) is the Bayesian answer
to the inverse problem, our knowledge of the model $u$ is encoded in the
probability measure $\mu^y$, which is fully specified by the density $\pi^y$.
There are several ways to characterise a probability distribution. Here we will
focus on the {\em Maximum A Posteriori (MAP)} estimator, \ie\ the element $u \in
X$ that maximises $\pi^y(u)$:
\begin{align}\label{eq:MAP}
  u^* = \arg\min_{u \in X} 
  \left(
  -\frac12 \left| y_0 - \mathcal G(u) \right|_C^2
  -\frac12 \left| u - u_0 \right|_{\Sigma_0}^2
  \right)\, .
\end{align}
For every instance of the data $y_0$, the MAP estimator is computed by
minimising a regulated $\chi^2$, we will refer to this procedure as the {\em
classical fit} of experimental data to a model. Note that in the Bayesian
approach, the regulator is provided by specifying carefully all the assumptions
that enter in the prior. In this specific example the regulator is a determined
by assuming that the model input $u$ is normally ditributed around a solution
$u_0$. The MAP estimator provides the explicit connection between the Bayesian
approach and the classical fit.

\subsection{Comparison with classical fitting}
\label{sec:comp-class-fit}

There are a number of results that make the connection between the two
approaches more quantitative, and therefore more transparent. We are
going to summarise these results here without proofs, referring the
reader to the mathematical literature for the missing details. Working
in the finite-dimensional case, we assume 
\begin{align*}
  u &\in \mathbb{R}^n \, ,\\
  y &\in \mathbb{R}^{\ndata}\, ,
\end{align*}
and we are going to consider in detail two examples from Ref.~\cite{StuartCore}.

\paragraph{Underdetermined system}
The first case that we are going to analyse is the case of a linear system that
is underdetermined by the data. The linear model is completely specified by a
vector of coefficients $g\in \mathbb{R}^n$, while we are going to assume that we
only have one datapoint, \ie\ $\ndata=1$, and hence:
\begin{equation}
  \label{eq:LinearModelEx}
  y = (g^T u) + \eta\, ,
\end{equation}
where $\eta \sim \mathcal{N}(0,\gamma^2)$ is one Gaussian number, whose
probability density is centred at $0$ and has variance $\gamma^2$. For
simplicity we are going to assume that the prior on $u$ is also a
multi-dimensional Gaussian, centred at $0$ with covariance matrix $\Sigma_0$. In
this case the posterior distribution can be written as
\begin{equation}
  \label{eq:GaussPostExplicit}
    \pi^y(u) \propto \exp \left[
    \frac{1}{2\gamma^2} \left|y - (g^T u) \right|^2 - \frac12 \left|
      u
    \right|_{\Sigma_0}^2 
    \right]\, ,
\end{equation}
which is still a Gaussian distribution for $u$. The mean and covariance are
respectively
\begin{align}
  m &= \frac{(\Sigma_0 g) y}{\gamma^2 + (g^T \Sigma_0 g)}\, , \\
  \Sigma &= \Sigma_0 - 
  \frac{(\Sigma_0 g) (\Sigma_0 g)^T}{\gamma^2 + (g^T \Sigma_0 g)}\, .
\end{align}
It is instructive to look at these quantities in the limit of infinitely precise data, \ie\ in the limit $\gamma\to 0$:
\begin{align}
  m_\star &= 
  \lim_{\gamma\to 0} m
  = \frac{(\Sigma_0 g) y}{(g^T \Sigma_0 g)}\, , \\
  \Sigma_\star &= 
  \lim_{\gamma\to 0} \Sigma 
  = \Sigma_0 - 
  \frac{(\Sigma_0 g) (\Sigma_0 g)^T}{(g^T \Sigma_0 g)}\, .
\end{align}
These values satisfy
\begin{align}
  (g^T m_\star) = y \, , \\
  (\Sigma_\star g) = 0 \, ,
\end{align}
which shows that the mean of the distribution is such that the data point is
exactly reproduced by the model, and that the uncertainty in the direction
defined by $g$ vanishes. It should be noted that the uncertainty in directions
perpendicular to $g$ does not vanish and is determined by a combination of the
prior and the model, \viz\ $\Sigma_0$ and $g$ in our example. This is a
particular example of a more general feature: for underdetermined systems the
information from the prior still shapes the probability distribution of the
solution even in the small noise limit.  

\paragraph{Overdetermined system}
We are now going to consider an example of an overdetermined system and discuss
again the case of small observational noise. We consider $\ndata\geq 2$ and
$n=1$, with a forward map such that
\begin{equation}
 \label{eq:OverDetForwMap}
 y = g(u + \beta u^3) + \eta\, ,
\end{equation} 
where $\eta$ is an $\ndata$-dimensional Gaussian variable with a diagonal
covariance $\gamma^2 I$, where $I$ denotes the identity matrix. For simplicity
we are going to assume a Gaussian prior with unit variance for $u$, which yields
for the posterior distribution:
\begin{equation}
  \label{eq:OverDetPost}
  \pi^y(u) \propto 
    \exp\left(
      -\frac{1}{2\gamma^2} \left| y - g(u + \beta u^3)\right|^2
      -\frac12 u^2
    \right)\, .
\end{equation} 
If $\beta=0$ the posterior is Gaussian and we can easily compute its mean and variance: 
\begin{align}
  m &= \frac{(g^T y)}{\gamma^2 + |g|^2} \, , \\
  \sigma^2 &=
    \frac{\gamma^2}{\gamma^2 + |g|^2}\, .
\end{align}
In this case, in the limit of vanishing observational noise, we obtain
\begin{align}
  m_\star &= \frac{(g^T y)}{|g|^2} \, ,\\
  \sigma_\star^2 &= 0\, .
\end{align}
The mean is given by the weighted average of the datapoints, which is also the solution of the $\chi^2$ minimization
\begin{equation}
  m_\star = \arg\min_{u\in\mathbb{R}} \left|y - g u\right|^2\, .
\end{equation}
Note that in this case the variance $\sigma_\star$ vanishes independently of the prior. In the limit of small noise, the distribution tends to a Dirac delta around the value of the MAP estimator.  


\subsection{The infinite-dimensional case}
\label{sec:infin-dimens-case}

In the finite-dimensional case, where the probability measures are
specified by their densities with respect to the Lebesgue measure,
Eq.~(\ref{eq:BayesThmInversePosterior}) encodes the fact that $\rho$
is the Radon-Nikodym derivative of the probability measure $\mu^y$
with respect to $\mu_0$, \viz
\begin{align}
  \label{eq:RadonNikodym}
  \frac{d\mu^y}{d\mu_0} (u) \propto \rho(y-\mathcal G(u)) \pi_0(u)\, .
\end{align}
Finally, using the fact that the density $\rho$ is a positive
function, we can rewrite 
\begin{align}
  \label{eq:PotentialDef}
  \rho(y-\mathcal G(u)) = \exp\left(-\Phi(u;y)\right)\, ,
\end{align}
and therefore
\begin{align}
  \label{eq:RadonNikodymTwo}
  \frac{d\mu^y}{d\mu^0} (u) \propto \exp\left(-\Phi(u;y)\right)\, .
\end{align}
In finite-dimensional spaces, the three equations above are just
definitions that do not add much content. Their interest resides in
the fact that the last expression, Eq.~(\ref{eq:RadonNikodymTwo}), can
be properly defined when $X$ is infinite-dimensional.


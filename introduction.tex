\section{Introduction}
\label{sec:Intro}

Inverse problems are the typical example of inference where a model is sought
starting from a finite-dimensional set of experimental observations. These
problems are notoriously difficult, and often require trying to guess a
continuous function, \ie\ an element of an infinite dimensional space, from a
finite amount of data. As emphasised by Hadamard a long time ago, it is easy to
end up in a situation where we deal with ill-posed problems, in the sense that
the solution may not exist, may not be unique, or may be unstable under small
variations of the input data. The determination of parton distributions from
experimental data, or the reconstruction of spectral densities from lattice QCD
simulations, are just two examples where these problems arise in particle
physics. In all these cases, finding a robust solution becomes very challenging,
if not impossible, making these questions all the more urgent, especially at a
time when precision studies are the ultimate challenge in order to highlight
divergences from the Standard Model. 

A Bayesian approach provides an apter tool for addressing inverse problems.
Starting from a prior distribution for the model, which encodes our theoretical
knowledge or prejudice, basic statistical tools allow us to determine the
posterior distribution of the solution, {\it after taking into account a set of
experimental observations.} The prior and posterior probabilities encode our
knowledge about the model before and after incorporating the experimental
results. There are multiple advantages to this formulation: the inverse problem
is well defined, the prior distribution ensures that all the assumptions are
explicit, and it allows to regulate the likelihood distribution. 

Note also that probability measures can be defined in infinite-dimensional
spaces, In cases where we are looking to determine a continuous function, the
Bayesian approach allows, at least in principle, to address the problem directly
in terms of the posterior probability measure of the model function. It is often
convenient for practical reasons to define a parametrization of the model
function in terms of a finite, albeit large, number of parameters and reduce the
problem to a finite dimensional one. The price to pay for this simplification is
the introduction of some bias, which needs to be understood and possibly
quantified. An expressive parametrization clearly helps in this case. The
Bayesian approach is also well suited to include potentially inconsistent data
in a systematic way. We will briefly touch upon this aspect below. 

A Bayesian approach to inverse problems has been actively developed by
mathematicians for a long time, and this development has accelerated quickly in
the last decade. In this paper we aim at summarising the existing framework and
adapt it to analyse the fits of parton distribution functions obtained by the
NNPDF collaboration. We will review the formalism in Sect. 2, trying to define
the notation that will be used in the rest of the paper. We will report some
examples already known, for illustrative purposes. We then try to connect the
Bayesian approach with the NNPDF fits based on a Monte Carlo methodology, where
the distribution of the PDFs is encoded in a set of fits to artificially
generated data, called replicas. We can anticipate here that, under the
hypotheses that the data are Gaussian and the model is linear, the NNPDF
procedure would characterise completely the posterior proabillity density. When
the model is non-linear two modifications need to be taken into account. First
of all the analytical calculation that we present in
Sect.~\ref{sec:BayesianInverse} is no longer possible, and one needs to rely on
the fact that a linearization of the model yields an accurate prediction. Even
though linear models are known to provide good approximations~\cite{xxx}, the
systematic errors introduced by this approximation are not easy to quantify.
There is also a more subtle effect that needs to be taken into account. When
working with linear models, the minimization procedure is know to have a unique
minimum, which can be computed exactly. Non-linear models can be plagued by
multiple minima, and more importantly by inefficiencies of the algorithm in
finding them. While it is important to be aware of the limitations of the
analytical calculation, it is also very useful to have an explicit result as a
template to guide the analysis of our numerical investigations. 

In the NNPDF fitting framework, the posterior probability of the parton
distribution functions, is encoded in an ensemble of fits to replicas of the
data, where the data replicas have been generated in order to reproduce the
fluctuations described by the experimental central values and uncertainties.
This bootstrap procedure propagates the data fluctuations in the space of fitted
PDFs. A successful fit should yield robust confidence intervals for observables,
in particular for those that are more relevant for phenomenology.

The idea of {\em closure tests} is to test this procedure in a fit to artificial
data that have been generated from a known set of input PDFs. In this case the
underlying law is known and we can check how the posterior distribution compares
to the underlying law. This is the basis of a closure test, which has already
been used to test to validity of previous iterations of the NNPDF methodology.
Here we aim to refine some of the pre-existing closure test estimators and with
the help of fast fitting methodology perform a more extensive study of how
faithful our uncertainties are. For this purpose we introduce new estimators
that allow us to quantify the quality of our fits. These esimators are defined
in the space of data, and we can use the Bayesian
formalism in order to compute analytically their probability distributions. 

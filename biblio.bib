@article{mlforphysics,
title = {A high-bias, low-variance introduction to Machine Learning for physicists},
journal = {Physics Reports},
volume = {810},
pages = {1-124},
year = {2019},
note = {A high-bias, low-variance introduction to Machine Learning for physicists},
issn = {0370-1573},
doi = {https://doi.org/10.1016/j.physrep.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0370157319300766},
author = {Pankaj Mehta and Marin Bukov and Ching-Hao Wang and Alexandre G.R. Day and Clint Richardson and Charles K. Fisher and David J. Schwab},
abstract = {Machine Learning (ML) is one of the most exciting and dynamic areas of modern research and application. The purpose of this review is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists. The review begins by covering fundamental concepts in ML and modern statistics such as the bias–variance tradeoff, overfitting, regularization, generalization, and gradient descent before moving on to more advanced topics in both supervised and unsupervised learning. Topics covered in the review include ensemble models, deep learning and neural networks, clustering and data visualization, energy-based models (including MaxEnt models and Restricted Boltzmann Machines), and variational methods. Throughout, we emphasize the many natural connections between ML and statistical physics. A notable aspect of the review is the use of Python Jupyter notebooks to introduce modern ML/statistical packages to readers using physics-inspired datasets (the Ising Model and Monte-Carlo simulations of supersymmetric decays of proton–proton collisions). We conclude with an extended outlook discussing possible uses of machine learning for furthering our understanding of the physical world as well as open problems in ML where physicists may be able to contribute.}
}

@article{nnpdf30,
    author = "Ball, Richard D. and others",
    collaboration = "NNPDF",
    title = "{Parton distributions for the LHC Run II}",
    eprint = "1410.8849",
    archivePrefix = "arXiv",
    primaryClass = "hep-ph",
    reportNumber = "EDINBURGH-2014-15, IFUM-1034-FT, CERN-PH-TH-2013-253, OUTP-14-11P, CAVENDISH-HEP-14-11",
    doi = "10.1007/JHEP04(2015)040",
    journal = "JHEP",
    volume = "04",
    pages = "040",
    year = "2015"
}

@article{ADVANI2020428,
title = {High-dimensional dynamics of generalization error in neural networks},
journal = {Neural Networks},
volume = {132},
pages = {428-446},
year = {2020},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2020.08.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608020303117},
author = {Madhu S. Advani and Andrew M. Saxe and Haim Sompolinsky},
keywords = {Neural networks, Generalization error, Random matrix theory},
abstract = {We perform an analysis of the average generalization dynamics of large neural networks trained using gradient descent. We study the practically-relevant âhigh-dimensionalâ regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that standard application of theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.}
}

@article{AbdulKhalek:2019ihb,
    author = "Abdul Khalek, Rabah and others",
    collaboration = "NNPDF",
    title = "{Parton Distributions with Theory Uncertainties: General Formalism and First Phenomenological Studies}",
    eprint = "1906.10698",
    archivePrefix = "arXiv",
    primaryClass = "hep-ph",
    reportNumber = "Edinburgh 2019/9, Nikhef/2019-014, TIF-UNIMI-2019-9 DAMTP-2019-24,
  CAVENDISH-HEP-19-11",
    doi = "10.1140/epjc/s10052-019-7401-4",
    journal = "Eur. Phys. J. C",
    volume = "79",
    number = "11",
    pages = "931",
    year = "2019"
}

@article{Ball:2018twp,
    author = "Ball, Richard D. and Nocera, Emanuele R. and Pearson, Rosalyn L.",
    collaboration = "NNPDF",
    title = "{Nuclear Uncertainties in the Determination of Proton PDFs}",
    eprint = "1812.09074",
    archivePrefix = "arXiv",
    primaryClass = "hep-ph",
    reportNumber = "Edinburgh 2018/4; Nikhef 2018-062, Edinburgh 2018/4, Nikhef 2018-062",
    doi = "10.1140/epjc/s10052-019-6793-5",
    journal = "Eur. Phys. J. C",
    volume = "79",
    number = "3",
    pages = "282",
    year = "2019"
}

@article{Ball:2020xqw,
    author = "Ball, Richard D. and Nocera, Emanuele R. and Pearson, Rosalyn L.",
    title = "{Deuteron Uncertainties in the Determination of Proton PDFs}",
    eprint = "2011.00009",
    archivePrefix = "arXiv",
    primaryClass = "hep-ph",
    reportNumber = "Edinburgh 2020/15, Nikhef/2020-033",
    doi = "10.1140/epjc/s10052-020-08826-7",
    journal = "Eur. Phys. J. C",
    volume = "81",
    number = "1",
    pages = "37",
    year = "2021"
}

@article{Faura_2020,
   title={The strangest proton?},
   volume={80},
   ISSN={1434-6052},
   url={http://dx.doi.org/10.1140/epjc/s10052-020-08749-3},
   DOI={10.1140/epjc/s10052-020-08749-3},
   number={12},
   journal={The European Physical Journal C},
   publisher={Springer Science and Business Media LLC},
   author={Faura, Ferran and Iranipour, Shayan and Nocera, Emanuele R. and Rojo, Juan and Ubiali, Maria},
   year={2020},
   month={Dec}
}

@article{Ball_2018,
   title={Precision determination of the strong coupling constant within a global PDF analysis},
   volume={78},
   ISSN={1434-6052},
   url={http://dx.doi.org/10.1140/epjc/s10052-018-5897-7},
   DOI={10.1140/epjc/s10052-018-5897-7},
   number={5},
   journal={The European Physical Journal C},
   publisher={Springer Science and Business Media LLC},
   author={Ball, Richard D. and Carrazza, Stefano and Debbio, Luigi Del and Forte, Stefano and Kassabov, Zahari and Rojo, Juan and Slade, Emma and Ubiali, Maria},
   year={2018},
   month={May}
}

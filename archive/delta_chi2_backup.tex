\section{Backup - delta chi2, for appendix?}

In NNPDF3.0 there was an estimator which aimed to measure
"over-fitting", $\Delta_{\chi^2}$, which using the notation here is defined as
\begin{equation}\label{eq:deltachi2def1}
    \Delta_{\chi^2} = \frac{1}{\ndata}\frac{(\erep{\diag{\model}} - \diag{\levone})_i^2 - \diag{\shift}_i^2}{\coveig_i},
\end{equation}
where we have normalised by $\ndata$ rather than in \cite{nnpdf30} which instead normalised
by $\sum_i^{\ndata} \frac{\diag{\shift}_i^2}{\coveig_i}$. It's clear that in the large
$\ndata$ limit that you would expect the two normalisations to be converge. The
idea of $\Delta_{\chi^2}$ is that it measures whether the central predictions
fit the level one data better than the underlying law. If $\Delta_{\chi^2}$
is negative then the central predictions overfit the level one data, whereas
if $\Delta_{\chi^2}$ is positive the central predictions underfit the level one
data. Ideally we would want to have $\Delta_{\chi^2}$ as close to zero as possible.
We can use the same trick of completing the square
\begin{equation}\label{eq:deltachi2def}
    \begin{split}
        \Delta_{\chi^2} &= \frac{1}{\ndata}\frac{(\erep{\diag{\model}} - \diag{\law} + \diag{\law} - \diag{\levone})_i^2 - \diag{\shift}_i^2}{\coveig_i} \\
        &= \frac{1}{\ndata}\frac{(\erep{\diag{\model}} - \diag{\law})_i^2 + (\diag{\law} - \diag{\levone})_i^2 + 2(\erep{\diag{\model}} - \diag{\law})_i(\diag{\law} - \diag{\levone})_i - \diag{\shift}_i^2}{\coveig_i} \\
        &= \bias - \shiftcross,
    \end{split}
\end{equation}
where in the second line the second and last terms cancel. The last line indicates
that $\Delta_{\chi^2}$ will likely appear in the decomposition of $\repchis$.
The first line of \eqref{eq:deltachi2def1} can be written as
\begin{equation}
    \Delta_{\chi^2} = \frac{1}{\ndata}\log \left[ \frac{p(\vv{\levone}| \vv{\law})}{p(\vv{\levone}| \erep{\vv{\model}})} \right]
\end{equation}
or the log of the conditional probability of getting the level one
data given the underlying law over the conditional probability of getting the
level one data given the central value of the model predictions. If we take the
expectation value across level one shifts
\begin{equation}\label{eq:deltachi2kldef}
    \begin{split}
        \eshift{\Delta_{\chi^2}} = \frac{1}{\ndata} \eshift{
            \log \left[ \frac{p(\vv{\levone}| \vv{\law})}{
                p(\vv{\levone}| \erep{\vv{\model}})} \right]}
    \end{split}
\end{equation}
then the expression takes the form of a KL divergence between the true conditional
probability distribution of the level one data given the underlying law and the
conditional probability of the level one data given the model predictions. There
is a feedback loop, because $\erep{\vv{\model}}$ is expected to have some
dependence on the shift.

We can actually define
a similar expression for each replica, and take the expectation value across
replicas:
\begin{equation}\label{eq:erepdeltachi2def}
    \erep{\Delta_{\repchis}} = \erep{\frac{1}{\ndata} \frac{(\diag{\model}^{\repind} - \diag{\levtwo}^{\repind})_i^2 - (\diag{\law} - \diag{\levtwo}^{\repind})_i^2}{\coveig_i}}
\end{equation}
in analogy to \eqref{eq:deltachi2def} if this quantity is negative, it's because
the average behaviour is that the replicas fit the level two data better than
the underlying law whereas if the quantity is positive, it's because the
level two replicas underfit the underlying law. By comparing the numerator with
some of the previously defined components of $\erep{\repchis}$ it's possible to
convince ourselves that
\begin{equation}
    \begin{split}
        \erep{\Delta_{\repchis}} &= \bias - \shiftcross + \var - \noisecross \\
        &= \Delta_{\chi^2} + \var - \noisecross,
    \end{split}
\end{equation}
where in the second line we reduced the first two terms using
\eqref{eq:deltachi2def}. The second term has a similar form, we define this as
\begin{equation}\label{eq:deltaepsdef}
    \begin{split}
        \deltaeps &= \var - \noisecross \\
        &= \frac{1}{\ndata}\erep{ \frac{(\diag{\model}^{\repind} - \erep{\diag{\model}})_i^2}{\coveig_i}
        - \frac{2\diag{\model_i}^{\repind} \noise_i}{\coveig_i}} \\
        &= \frac{1}{\ndata}\erep{ \frac{(\diag{\model}^{\repind} - \erep{\diag{\model}})_i^2}{\coveig_i}
        - \frac{2(\diag{\model_i}^{\repind} - \erep{\diag{\model}}) \noise_i}{\coveig_i}},
    \end{split}
\end{equation}
where in the last line we used the fact that a term which is linear in $\noise$
will have an expectation value across replicas of zero. Qualitatively, the last
line of equation \eqref{eq:deltaepsdef} is the variance of the model replica
predictions, $\model^{\repind}$ minus two times the covariance of the model
replica predictions and the pseudodata replicas. This makes sense, the expression
tells us whether the replica predictions fluctuate about their mean covariantly
with the pseudodata replicas. If we take the expectation of \eqref{eq:erepdeltachi2def}
across fits, then we will obtain a similar expression to
\eqref{eq:deltachi2kldef} - a KL divergence between the conditional probabilities
of the data given the underlying law and model replicas respectively.

Now consider the case where $\erep{\model}$ is exactly
equal to $\law$ but the difference between the replica predictions and the
central replica predictions is equal to the noise applied to the corresponding
pseudodata replica. Then it's clear that $\Delta_{\chi^2}=0$ and $\deltaeps=-1$
so we have maximal overfitting of the noise. Likewise if every replica were to
exactly pass through the level one data, $\levone$, then $\Delta_{\chi^2}=-1$
but $\deltaeps=0$. These two examples demonstrate how the two terms can combine
to give the overall picture of whether the model replicas overfit the level two
data.

\subsection{some more}



In \eqref{eq:chi2decomp}, we implictly showed that $\Delta_{\chi^2}$ discussed
in the closure test of NNPDF3.0 is given by
\begin{equation}
    \Delta_{\chi^2} = {\rm bias} - {\rm cross\,term}.
\end{equation}
In the NNDPF3.0 paper, $\Delta_{\chi^2}$ was said to indicate under or overfitting
depending on its sign. Also it was said that $\Delta_{\chi^2}$ indicated that
the underlying law was reproduced. We can see with this decomposition that
if the underlying law is exactly obtained by the central value of the predictions,
$\Delta_{\chi^2}=0$, however $\Delta_{\chi^2}=0$ does not necessarily imply that
the underlying law is reproduced. On figure \ref{fig:diagram2destimators},
$\Delta_{\chi^2} = 0$ would define a hypersphere which is centered on the level
1 data and passes through the underlying law (so has radius equal to magnitude of
the vector from $\law$ to $\levone$). If $\Delta_{\chi^2} < 0$ then the central
value of the predictions lies within this hypersphere, similarly if
$\Delta_{\chi^2} > 0$ then the central value of the predictions lies outside
of this hypersphere. We should bear this in mind when talking about overfitting
with regards to both $\Delta_{\chi^2}$ and $\deltaeps$, we are specifically
defining that type of overfitting as fitting the fluctuated data better than
the underlying law. In the large $\ndata$ limit this manifests itself as
having enough freedom in the parameterisation that the minimum of the likelihood
function is lower than the expected value if the data were generated from that
model, this would be the case if $\chi^2 < 1$
(in a statistical significant way) or $\erep{\repchis} < 2$ for
$\Delta_{\chi^2}$ and $\erep{\Delta_{\repchis}}$ respectively. As previously discussed,
in the latter case there is an interplay between the overfitting of the central
predictions and the overfitting of the level two noise $\noise$ so it becomes
advantageous to consider $\Delta_{\chi^2}$ and $\deltaeps$ separately.

\section{Understanding NNPDF3.0 data estimators}

In the closure test presented in NNPDF3.0 \cite{nnpdf30} there was a data-space
estimator which aimed to measure the level of over or under fitting, $\deltachi$.
Here we discuss how $\deltachi$ can emerge from the bias-variance decomposition
and then use the linear model to try and understand it in the context of
viewing the ensemble of model replicas as a sample from the posterior distribution
of the model given the data.

Despite the link between the estimators emerging from the decomposition of
$\eout$ and the posterior distribution for data which is not used to inform
the model parameters, if we perform the same decomposition as in
Sec.~\ref{sec:ClosureEstimatorsDerivation} but set
$\testset{\obspriorcent}=\obspriorcent$ then we find that the cross term
in the final line of Eq.~\ref{eq:EoutDecomposition} does not go to zero when
the expectation across data is taken because there is a dependence on
$\obspriorcent$ in both the model predictions and the noisey data. As a result
we have to modify Eq.~\ref{eq:ExpectedBiasVariance} to be
\begin{align}\label{eq:ExpectedBiasVarianceTraining}
    \mathbf{E}_{\obspriorcent}[\ein] &=
    \mathbf{E}_{\obspriorcent}[\bias] + 
    \mathbf{E}_{\obspriorcent}[\var] \nonumber \\ 
    &+\mathbf{E}_{\obspriorcent}[{\rm noise}] +
    \mathbf{E}_{\obspriorcent}[\noisecross]\, ,
\end{align}
where we refer now to the right hand side of
Eq.~\ref{eq:ExpectedBiasVarianceTraining} as $\ein$ because it's evaluated on the
data used to inform the model replicas.

Now we examine the definition of $\deltachi$ introduced
in~\cite{nnpdf30}, defined as the difference between the
$\chi^2$ between the expectation value of the model predictions and the level
one data, and the $\chi^2$ between the underlying observable values and the
level one data. In~\cite{nnpdf30} the denominator was also set to be the
second term in the numerator, however here we slightly re-define
$\deltachi$ to instead simply be normalised by the number of data points:
\begin{equation}\label{eq:deltachi2def1}
    \begin{split}
        &\deltachi = 
            \frac{1}{\ndata}\times \\ 
            & \Big[ \left( \emodel{\fwdobsop\left(\modelvecrep\right)} - \obspriorcent \right)^T
            \obspriorcov^{-1}
            \left( \emodel{\fwdobsop\left(\modelvecrep\right)} - \obspriorcent \right) \\
            & \quad - \left( \law - \obspriorcent \right)^T
            \obspriorcov^{-1}
            \left( \law - \obspriorcent \right)
        \Big] \\
        &= \bias + \noisecross \, ,
    \end{split}
\end{equation}
where in the second line we show how $\deltachi$ itself can be decomposed to
be equal to two of the terms in Eq.~\ref{eq:ExpectedBiasVarianceTraining}.

Constant values of $\deltachi$ define elliptical contours in data space
centered on the level one data. $\deltachi = 0$, in particular, defines a
contour which is centered on the level one data and passes through the
underlying law. When viewing $\deltachi$ from a classical fitting perspective,
if $\deltachi < 0$ then the expectation value of the model
predictions fit the level one data better than the underlying observables -
which indicates an overfitting of the shift, $\boldsymbol{\shift}$. Similarly,
$\deltachi > 0$ indicates some underfitting of the level one data.

If we return to the linear model we can write the analytic value of
$\deltachi$. Firstly, since $\testset{\obspriorcent}=\obspriorcent$ we can
simplify Eq.~\ref{eq:BiasLinearModel}
\begin{equation}\label{eq:BiasLinearModelSimple}
    \begin{split}
        &\mathbf{E}_{\obspriorcent}[{\rm bias}] = \frac{1}{\ndata}
            {\rm Tr} \left[
                \linmap \modelpostcov \linmap^T \obspriorcov^{-1}
            \right] \\
            &\quad= \frac{1}{\ndata}{\rm Tr} \left[ \modelpostcov \modelpostcov^{-1}\right] \\
            &\quad= \frac{\nmodel}{\ndata} \, ,
    \end{split}
\end{equation}
because $\modelpostcov \modelpostcov^{-1}$ is an $\nmodel \times \nmodel$ identity
matrix. Similarly we can write down the cross term
\begin{equation}
    \begin{split}
        \mathbf{E}_{\obspriorcent}[\noisecross] &= \frac{-2}{\ndata} \mathbf{E}_{\obspriorcent} \left[
            (\linmap \modelpostcov \linmap^T \obspriorcov^{-1} \obsnoise)^T \obspriorcov^{-1} \obsnoise
        \right] \\
        &= \frac{-2}{\ndata}{\rm Tr} [\linmap \modelpostcov \linmap^T \obspriorcov^{-1}] \\
        &= -2 \frac{\nmodel}{\ndata}
    \end{split}
\end{equation}
which leaves us with
\begin{equation}
    \mathbf{E}_{\obspriorcent}[\deltachi] = - \frac{\nmodel}{\ndata} \, .
\end{equation}
The point is that the linear model has already been shown to be a sample from
posterior distribution of the model given the data. But from the classical
fitting point of view we would say this model has overfitted.

As such, we do not report any results with $\deltachi$ here, because
when $\biasvarratio = 1$, it doesn't add much to the discussion. It may still
be useful as a diagnostic tool when $\biasvarratio \neq 1$, which as discussed
could be for a variety of reasons - including fitting inefficiency. It also
may be used as a performance indicator for deciding between two fitting
methodologies: if both fits are shown to have $\biasvarratio = 1$, the
methodology with smaller magnitude of $\deltachi$ could be preferential.
The same could be said for bias and variance however, bias in particular is
clearly closely related to $\deltachi$.

\section{Introduction}
\label{sec:Intro}

Inverse problems are the typical example of inference where a model is sought
starting from a finite-dimensional set of experimental observations. These
problems are notoriously difficult, and often require trying to guess a
continuous function, \ie\ an element of an infinite dimensional space, from a
finite amount of data. As emphasised by Hadamard a long time ago, it is easy to
end up in a situation where we deal with ill-posed problems, in the sense that
the solution may not exist, may not be unique, or may be unstable under small
variations of the input data. The determination of parton distributions from
experimental data, or the reconstruction of spectral densities from lattice QCD
simulations, are just two examples where these problems arise in particle
physics. In all these cases, finding a robust solution becomes very challenging,
if not impossible, making these questions all the more urgent, especially at a
time when precision studies are the ultimate challenge in order to highlight
divergences from the Standard Model. 

A Bayesian approach provides an apter tool for addressing inverse problems.
Starting from a prior distribution for the model, which encodes our theoretical
knowledge or prejudice, basic statistical tools allow us to determine the
posterior distribution of the solution, {\it after taking into account a set of
experimental observations.} The prior and posterior probabilities encode our
knowledge about the model before and after incorporating the experimental
results. There are multiple advantages to this formulation: the inverse problem
is well defined, and the prior distribution ensures that all the assumptions are
explicit, while regulating the likelihood distribution. 

Note also that probability measures can be defined in infinite-dimensional
spaces. In cases where we are looking to determine a continuous function, the
Bayesian approach provides, at least in principle, a formulation of the problem
directly in terms of the posterior probability measure of the model function. It
is often convenient for practical reasons to define a parametrization of the
model function in terms of a finite, albeit large, number of parameters and
reduce the problem to a finite dimensional one. The price to pay for this
simplification is the introduction of some bias, which needs to be understood
and possibly quantified. An expressive parametrization clearly helps in this
case. 

A Bayesian approach to inverse problems has been actively developed by
mathematicians for a long time, and this development has dramatically
accelerated in the last decade. In this paper we aim at summarising the existing
framework and adapt it to analyse the fits of parton distribution functions
obtained by the NNPDF collaboration. We review the Bayesian formalism in
Sec.~\ref{sec:inverse-problems}, where we define the notation that will be used
in the rest of the
paper. We report some known examples for illustrative purposes. Even though
these are well-known results, we find it useful to summarise them in the context
of our specific problem. In Sec.~\ref{sec:closure-test}, we try to connect the
Bayesian approach
with the NNPDF fits based on a Monte Carlo methodology, where the distribution
of the PDFs is encoded in a set of fits to artificially generated data, called
replicas. We can anticipate here that, under the hypotheses that the data are
Gaussian and the model is linear, the NNPDF procedure does characterise
completely the posterior proabillity density. When the model is non-linear two
modifications need to be taken into account. First of all the analytical
calculation that we present in Sec.~\ref{sec:BayesianInverse} is no longer
possible, and one needs to rely on the fact that a linearization of the model
yields an accurate prediction. Even though linear models are known to provide
good approximations, the systematic errors introduced by this approximation are
not easy to quantify. There is also a more subtle effect that needs to be taken
into account. When working with linear models, the minimization procedure is
know to have a unique minimum, which can be computed exactly. Non-linear models
can be plagued by multiple minima, and more importantly by inefficiencies of the
algorithm in finding them. While it would be foolish to ignore the limitations
of the analytical calculation, it is nonetheless very useful to have an explicit
result as a template to guide the analysis of our numerical investigations. 

In the NNPDF fitting framework, the posterior probability of the parton
distribution functions, is encoded in an ensemble of fits to replicas of the
data, where the data replicas have been generated in order to reproduce the
fluctuations described by the experimental central values and uncertainties.
This bootstrap procedure propagates the data fluctuations in the space of fitted
PDFs. A successful fit should yield robust confidence intervals for observables,
in particular for those that are more relevant for phenomenology.

The idea of {\em closure tests} is to test this procedure in a fit to artificial
data that have been generated from a known set of input PDFs. In this case the
underlying law is known and we can check how the posterior distribution compares
to the underlying law. This is the basis of a closure test, which is summarised
at the end of Sec.~\ref{sec:closure-test}. Closure tests have already been used
to test to validity
of previous iterations of the NNPDF methodology. Here we aim to refine some of
the pre-existing closure test estimators and with the help of fast fitting
methodology perform a more extensive study of how faithful our uncertainties
are. 
For this purpose we introduce in Sec.~\ref{sec:ClosureEstimators} new
estimators that allow us to
quantify the quality of our fits. These estimators are defined in the space of
data and need to be understood as stochastic variables that are characterised by
a probability density. Where possible, we use the Bayesian formalism in order to
compute analytically these probability densities and compare with numerical
results. In order to perform analytical calculations we often need to make some
simplifying assumptions. While it is incorrect to use the analytical results for
making quantitative predictions for the results of the realistic case of a
non-linear fit, the analytical results provide a guide to the interpretation of
the observed patterns.
It is important to stress that what we test in a closure test in terms of faithfulness 
does not fully correspond to the real life situation:
the artificial data used in a fit are by construction compatible between each 
other, because generated starting from the same underlying law; 
this might not be the case when considering global fits on real data, where tensions
between different datasets can be present.  

The results of our numerical studies are summarised in
Sec.~\ref{sec:numerical-results}, where we also
compare the NNPDF4.0 methodology introduced in the latest NNPDF
fit~\cite{NNPDF40} to the methodology used by the collaboration in previous
fits. 
 
The Bayesian formalism, by providing posterior probability distributions, paves
the way to explore a number of issues. We highlight in our conclusions some
possible questions that we defer to future studies. 


\section{Linear Forward Map}

\subsection{Closure estimators - Linear forward map}

Once again we return to the linear model framework set out in
Sec.~\ref{sec:fluct-fit-values}. We can perform an analytical closure
test in this framework, and check our
understanding of the closure estimators. Consider the true observable
values for the test data is given by
\begin{equation}\label{eq:LinearLawMap}
    \testset{\law} = \testset{\fwdobsop} \lawmodel
\end{equation}
where $\lawmodel \in \modelspace$, which means the number of parameters
in the underlying law is less than or equal to the number of parameters in the
model, $\nlaw \leq \nmodel$. Using the previous results from
Sec.~\ref{sec:fluct-fit-values}, we can write down the
difference between the true observables and the predictions from the MAP estimator
(or the expectation of the model predictions across model replicas - in the
linear model these are the same)
\begin{equation}
    \begin{split}
        \emodel{\testset{\fwdobsop}\left(\modelvecrep\right)} - \testset{\law} &=
        \testset{\linmap} (\lawmodel - \linmap^+ \obspriorcent ) \\
        &= \testset{\linmap} \linmap^+ \obsnoise \, ,
    \end{split}
\end{equation}
where we recall that $\linmap$ is the forward map to the training observables
and $\obspriorcent$ are
the corresponding training central values. Calculating the covariance across
training data of
$\emodel{\testset{\fwdobsop}\left(\modelvecrep\right)} - \testset{\law}$
gives
\begin{equation}
    \covcent = \testset{\linmap} \linmap^+ \obspriorcov (\testset{\linmap} \linmap^+)^T \, ,
\end{equation}
so the full expression for $\mathbf{E}_{\obspriorcent}[{\rm bias}]$ is given by
\begin{equation}
    \mathbf{E}_{\obspriorcent}[{\rm bias}] = \frac{1}{\ndata}
    {\rm Tr} \left[
        \testset{\linmap} \linmap^+ \obspriorcov (\testset{\linmap} \linmap^+)^T
        \testset{\modelpriorcov}^{-1}
    \right].
\end{equation}
We note that if the test data is identical the data the model was fitted on,
we recover an intuitive result $\mathbf{E}_{\obspriorcent}[{\rm bias}] = \frac{\nmodel}{\ndata}$.
Consider the example of the polynomial, the maximum value which $\nmodel$ can
take whilst $\linmap$ still has linearly independent rows is $\ndata$ and in this case
the $\mathbf{E}_{\obspriorcent}[{\rm bias}]$ takes it's maximum value of 1. The central
predictions from the model exactly pass through each data point.

We can perform a similar exercise on the model replica predictions. The difference
between the predictions from model replica $\repind$ and the expectation value
of the model predictions is
\begin{equation}
    \begin{split}
        \testset{\fwdobsop}\left(\modelvecrep\right) -
        \emodel{\testset{\fwdobsop}\left(\modelvecrep\right)} &=
        \testset{\linmap} (\linmap^+ \pseudodat^{\repind} - \linmap^+ \modelpriorcent) \\
        &= \testset{\linmap} \linmap^+ \noise^{\repind}.
    \end{split}
\end{equation}
Since $\noise$ and $\obsnoise$ follow the same distribution, it's clear that
\begin{equation}
    \covrep = \covcent,
\end{equation}
which, as a result means that
\begin{equation}
    \var = \mathbf{E}_{\obspriorcent}[{\rm bias}].
\end{equation}
We recall that when the map is linear, the NNPDF MC methodology generates replicas
which are sampled from the posterior distribution of the model given the data.
We have shown here that provided the underlying law belongs to the model
space, the posterior distribution of the model predictions satisfy the
requirement that $\biasvarratio = 1$.

\subsection{Relating variance to posterior distribution}

If the training and test split is ideal, as discussed in
Sec.~\ref{sec:ClosureEstimatorsDerivation}, such that the two sets of data
are uncorrelated then it's
possible to perform a sequential marginalisation of the data \cite{tarantola}.
The posterior model distribution obtained by marginalising over the training
data, can be used as a prior when viewing the test data from a Bayesian
perspective. The model prior for the test data is given by
Eq.~\ref{eq:PosteriorMultiGaussModel}, which is fully
parameterised by mean and covariance: $\modelpriorcent = \linmap^+ \obspriorcent$ and
$\modelpriorcov = \linmap^+ \obspriorcov (\linmap^+)^T$ respectively.
The posterior data-space distribution
of the new data is given by Eq.~\ref{eq:PosteriorDataSpace}, which is also a
Gaussian with mean and covariance given by Eq.~\ref{eq:PosteriorDataParams}.
Now if we take $\testset{\obspriorcov} \to \infty$, which is equivalent to
there being prior information for the test data, the posterior data-space distribution
is given by
\begin{equation}
    \begin{split}
        \pi_D(\testset{\obspriorcent} |\obspriorcent, \modelpriorcent, \linmap)
        &\propto \exp \left( - \frac{1}{2}
            (\testset{\obspriorcent} - \testset{\linmap} \linmap^+ \modelpriorcent)^T
            {{\testset{\linmap}}^T}^+ \linmap^T \obspriorcov \linmap {\testset{\linmap}}^+
            (\testset{\obspriorcent} - \testset{\linmap} \linmap^+ \modelpriorcent)
        \right) \, ,
    \end{split}
\end{equation}
a Gaussian with covariance
$\left( {{\testset{\linmap}}^T}^+ \linmap^T \obspriorcov \linmap {\testset{\linmap}}^+ \right)^{-1}$
and mean $\testset{\linmap} \linmap^+ \modelpriorcent$. We recognise that the
covariance of posterior
distribution of the test data, is the covariance of model predictions for the
test data, $\covrep$. Taking $\testset{\obspriorcov} \to \infty$ might seem
slightly peculiar, however we can understand this as treating the test dataset
as new observables for which there are no experimental measurements. When comparing
$\covcent$, the covariance of the MAP estimators, to $\covrep$ we are
checking that the observed fluctuation of the MAP estimators about the true
observables is consistent with the posterior data-space covariance.

\subsection{Underparameterised model}

Note that if we were to choose the
number of model parameters such that $\nlaw > \nmodel$, then the variance
would be unaffected, since the underlying law parameters cancel. However, the
bias would now contain an extra term from the extra parameters in the
underlying law, schematically:
\begin{equation}
    \begin{split}
        (\emodel{\testset{\fwdobsop}\left(\modelvecrep\right)} - \testset{\law})_i =
        \sum_{1 \leq j \leq \nmodel} \testset{\linmap}_{ij} (\linmap^+ \obsnoise)_j +
        \sum_{\nmodel < j \leq \nlaw} \testset{\linmap}_{ij} \lawmodel_j,
    \end{split}
\end{equation}
which would mean that $\biasvarratio \neq 1$. This demonstrates that requiring
$\biasvarratio = 1$ demands that the model space is suitably flexible, if the
underlying law is parameterised then this can be summarised as requiring
$\lawmodel \in \modelspace$. Note that in the
underparameterised regime the model replicas are still drawn from the posterior
distribution, however because $\lawmodel \notin \modelspace$ we've somehow
invalidated the assumptions that go into the relation between model predictions
and the data-space prior.

Although $\biasvarratio$ was largely chosen on practical
grounds, we see that it is still a stringent test that our assumptions are
correct and that the distribution our model replicas are drawn from is meaningful,
this is what we mean when we say {\em faithful uncertainties}.

An unfortunate
trade-off when using $\biasvarratio$ is it can't be used as a diagnostic
tool, and is instead used simply for validation. For example, if
$\biasvarratio > 1$, then we
can't know whether there was a problem with the fitting methodology used to
generate the model replicas or a deeper issue such as an inflexible model.

\section{Neural network parton distribution functions}
% TODO: here we should properly define the PDFs, forward map and which experimental
% data we fitted and which was used in the test set.

When fitting experimental data we vary the parameters of a set of PDF replicas
at the initial scale such that the $\chi^2$ is minimised between the
corresponding theory predictions and a generated pseudodata replica. A set of
PDFs usually refers to a set of seperate continuous functions, one for each
flavour of PDF in a particular basis. In this specific study, fits performed
parameterise the set of PDFs as a single neural network which takes
as input $x$ and $\ln x$ and returns 8 outputs, one for each flavour in the
fitting basis, multiplied by some preproccessing exponents. The output for a
single flavour $j$ is
\begin{equation}
    NN(x, \ln x)_j * x^{1-\alpha_j} * (1-x)^{\beta_j},
\end{equation}
where each flavour has it's own preproccessing exponents $\alpha$ and $\beta$,
parameters that are varied in these fits, and $NN(x, \ln x)_j$ is the
$j^{\rm th}$ output from the neural network.
When an experiment is included in an NNPDF fit, we take the published
experimental central values and uncertainties (statistical and systematic)
and use these pieces of information to generate the pseudodata.
The pseudodata replica is generated
through Monte Carlo sampling by applying noise to the experimental
central values.
After fitting many sets of PDF replicas (usually of order 100 sets),
each set to an independently generated pseudodata replica, we have an ensemble of
PDF replicas.

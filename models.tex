\section{Linear Forward Map}

\subsection{Closure estimators}

In Sec.~\ref{sec:closure-test} a model whose predictions are linear in the
model parameters was introduced - which we will refer to as a linear forward map.
We can perform an analytical closure test in this framework, and check that our
understanding of the closure estimators is correct. Consider the true observable
values being generated from an input vector of parameters $c$, such that
\begin{equation}\label{eq:LinearLawMap}
    f^\prime = X^\prime c
\end{equation}
where $c$ belongs to the model space, in this case that means the number of parameters
in the underlying law is less than or equal to the number of parameters in the
model, $\nlaw \leq \nmodel$. Combining Eqs.~\ref{eq:LinearLawMap},
\ref{eq:LinForwardMap} and \ref{eq:MAPEstLinModel}, we can write down the
difference between the true observables and the expectation of the model
predictions
\begin{equation}
    \begin{split}
        \erep{\model(u^{* \repind})} - f^\prime &= X^\prime (c - X^+ \levone ) \\
        &= X^\prime X^+ \shift
    \end{split}
\end{equation}
where $X$ is the forward map to the training predictions and $\levone$ are
the corresponding training central values. Calculating the covariance across
training data of $\erep{\model(u^{* \repind})} - f^\prime$ gives
\begin{equation}
    \covcent = X^\prime X^+ \cov(X^\prime X^+)^T,
\end{equation}
so the full expression for $\mathbf{E}_{\levone}[{\rm bias}]$ is given by
\begin{equation}
    \mathbf{E}_{\levone}[{\rm bias}] = \frac{1}{\ndata}
    {\rm Tr} \left[ X^\prime X^+ \cov (X^\prime X^+)^T {\cov^{\prime}}^{-1} \right].
\end{equation}
We note that if the test data is identical the data the model was fitted on,
we recover an intuitive result $\mathbf{E}_{\levone}[{\rm bias}] = \frac{\nmodel}{\ndata}$.
Consider the example of the polynomial, the maximum value which $\nmodel$ can
take whilst $X$ still has linearly independent rows is $\ndata$ and in this case
the $\mathbf{E}_{\levone}[{\rm bias}]$ takes it's maximum value of 1. The central
predictions from the model exactly pass through each data point.

We can perform a similar exercise on the model replica predictions. The difference
between the predictions from model replica $\repind$ and the expectation value
of the model predictions is
\begin{equation}
    \begin{split}
        \model(u^{* \repind}) - \erep{\model(u^{* \repind})} &= X^\prime (X^+ \levtwo^{\repind} - X^+ \levone) \\
        &= X^\prime X^+ \noise^{\repind}.
    \end{split}
\end{equation}
Since $\noise$ and $\shift$ follow the same distribution, it's clear that
\begin{equation}
    \covrep = \covcent,
\end{equation}
which, as a result means that
\begin{equation}
    \var = \mathbf{E}_{\levone}[{\rm bias}].
\end{equation}
We recall that when the map is linear, the NNPDF MC methodology generates replicas
which are sampled from the posterior distribution of the model given the data.
We have shown here that provided the underlying law belongs to the model
space, the posterior distribution of the model predictions satisfy the
requirement that $\biasvarratio = 1$.

\subsection{Relating variance to posterior distribution}

When we consider data which was not used to inform the model distribution,
denoted throughout this section with prime notation, then we can consider the posterior
distribution from the training data, $\levone$ as a prior for the unseen data.
The model prior is given by Eq.~\ref{eq:PosteriorMultiGaussModel}, which is fully
parameterised by mean and covariance: $u_0 = X^+ z$ and
$C_M^{-1} = X^T\invcov{D}X$ respectively. The posterior data-space distribution
of the new data is given by Eq.~\ref{eq:PosteriorDataSpace}, which is also a
Gaussian with mean and covariance given by Eq.~\ref{eq:PosteriorDataParams}.
Now if we take ${\cov^{\prime}} \to \infty$, which is equivalent to there being
no data for these predictions yet, the posterior data-space distribution
is given by
\begin{equation}
    \begin{split}
        \pi_D(\levone^\prime|\levone,u_0,\mathcal{G}) &\propto \exp
        \left( - \frac{1}{2}
            (\levone^\prime - X^\prime X^+ \levone)^T
            {{X^\prime}^T}^+ X^T \invcov{} X {X^\prime}^+
            (\levone^\prime - X^\prime X^+ \levone)
        \right) \, ,
    \end{split}
\end{equation}
a Gaussian with covariance
$\covrep = \covcent = \left({{X^\prime}^T}^+ X^T \invcov{} X {X^\prime}^+ \right)^{-1}$
and mean $X^\prime X^+ \levone$. Comparing $\covrep$ and $\covcent$ can be viewed
as checking the consistency with the distribution of MAP estimators about the
true observable values and the covariance of the posterior distribution
in data-space of unseen data.

\subsection{Underparameterised model}

Note that if we were to choose the
number of model parameters such that $\nlaw > \nmodel$, then the variance
would be unaffected, since the underlying law parameters cancel. However, the
bias would now contain an extra term from the extra parameters in the
underlying law, schematically:
\begin{equation}
    \begin{split}
        \erep{\model(u^{* \repind})} - f^\prime =
        X^\prime X^+ \shift + X^\prime_{|c| > \nmodel} c_{|c| > \nmodel},
    \end{split}
\end{equation}
which would mean that $\biasvarratio \neq 1$. We now see that requiring
$\biasvarratio = 1$ demands that the model space is suitably flexible. In the
underparameterised regime the model replicas are still drawn from the posterior
distribution, however because $\vert c \vert > \vert u \vert$
%TODO: call model space something different so we just say c not in model space
we've somehow invalidated our fitting procedure, which is reflected in the
data space estimator.

Although $\biasvarratio$ was largely chosen on practical
grounds, we see that it is still a stringent test that our assumptions are
correct and that the distribution our model replicas are drawn from is meaningful,
this is what we mean when we say {\em faithful uncertainties}.

An unfortunate
trade-off when using $\biasvarratio$ is it can't be used as a diagnostic
tool, and is instead used simply for validation. For example, if
$\biasvarratio > 1$, then we
can't know whether there was a problem with the distribution of the replicas
or a deeper issue that the model is underparameterised.

\section{Neural network parton distribution functions}
% TODO: here we should properly define the PDFs, forward map and which experimental
% data we fitted and which was used in the test set.

When fitting experimental data we vary the parameters of a set of PDF replicas
at the initial scale such that the $\chi^2$ is minimised between the
corresponding theory predictions and a generated pseudodata replica. A set of
PDFs usually refers to a set of seperate continuous functions, one for each
flavour of PDF in a particular basis. In this specific study, fits performed
with \nfit\ parameterise the set of PDFs as a single neural network which takes
as input $x$ and $\ln x$ and returns 8 outputs, one for each flavour in the
fitting basis, multiplied by some preproccessing exponents. The output for a
single flavour $j$ is
\begin{equation}
    NN(x, \ln x)_j * x^{1-\alpha_j} * (1-x)^{\beta_j},
\end{equation}
where each flavour has it's own preproccessing exponents $\alpha$ and $\beta$,
parameters that are varied in these fits, and $NN(x, \ln x)_j$ is the
$j^{\rm th}$ output from the neural network.
When an experiment is included in an NNPDF fit, we take the published
experimental central values and uncertainties (statistical and systematic)
and use these pieces of information to generate the pseudodata.
The pseudodata replica is generated
through Monte Carlo sampling by applying noise to the experimental
central values.
After fitting many sets of PDF replicas (usually of order 100 sets),
each set to an independently generated pseudodata replica, we have an ensemble of
PDF replicas.
